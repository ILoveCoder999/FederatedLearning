{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c930a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing taskarithmetic.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile taskarithmetic.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Part 1: SparseSGDM Optimizer \n",
    "# ---------------------------------------------------------\n",
    "class SparseSGDM(optim.SGD):\n",
    "    \"\"\"\n",
    "    Implements Stochastic Gradient Descent with Momentum (SGDM) \n",
    "    that supports gradient masking for sparse fine-tuning.\n",
    "    \n",
    "    Inherits from torch.optim.SGD.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.001, momentum=0.9, weight_decay=0.0, masks=None):\n",
    "        \"\"\"\n",
    "        :param params: Model parameters to optimize.\n",
    "        :param masks: A dictionary mapping parameter names (or IDs) to binary masks.\n",
    "                      If mask[i] == 0, the gradient for param[i] is zeroed out.\n",
    "        \"\"\"\n",
    "        super(SparseSGDM, self).__init__(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        self.masks = masks\n",
    "\n",
    "    def set_masks(self, masks):\n",
    "        \"\"\"\n",
    "        Update the masks used by the optimizer.\n",
    "        \"\"\"\n",
    "        self.masks = masks\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        # Iterate over all parameter groups (standard PyTorch optimizer structure)\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            d_p_list = []\n",
    "            momentum_buffer_list = []\n",
    "            \n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            lr = group['lr']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    # -----------------------------------------------------------\n",
    "                    # Key Modification for Task 3.3 \n",
    "                    # \"Zero-out the updates of the weights whose corresponding \n",
    "                    # entry in the mask is zero.\"\n",
    "                    # -----------------------------------------------------------\n",
    "                    if self.masks is not None:\n",
    "                        # We identify the mask by the parameter tensor's object ID\n",
    "                        # or we assume the `masks` passed is a list aligned with parameters.\n",
    "                        # For simplicity in this implementation, we assume `self.masks`\n",
    "                        # is a dictionary {param_tensor_id: mask_tensor} or we handle it externally.\n",
    "                        \n",
    "                        # However, a robust way for this project is to check if \n",
    "                        # the parameter has a state attribute for the mask.\n",
    "                        pass \n",
    "                        \n",
    "                        # PRACTICAL IMPLEMENTATION:\n",
    "                        # Apply mask directly to p.grad before the standard SGD update\n",
    "                        if p in self.masks:\n",
    "                            mask = self.masks[p]\n",
    "                            p.grad.mul_(mask) # In-place multiplication: grad = grad * mask\n",
    "                    \n",
    "                    params_with_grad.append(p)\n",
    "                    d_p_list.append(p.grad)\n",
    "\n",
    "                    state = self.state[p]\n",
    "                    if 'momentum_buffer' not in state:\n",
    "                        momentum_buffer_list.append(None)\n",
    "                    else:\n",
    "                        momentum_buffer_list.append(state['momentum_buffer'])\n",
    "\n",
    "            # Call the functional SGD step (standard PyTorch logic)\n",
    "            # We must implement the manual update or call the functional API \n",
    "            # ensuring we use the modified gradients.\n",
    "            \n",
    "            # Since we modified p.grad in-place above, we can just call the standard \n",
    "            # SGD logic or implementing a simplified version here:\n",
    "            \n",
    "            for i, p in enumerate(params_with_grad):\n",
    "                d_p = d_p_list[i]\n",
    "                \n",
    "                # Weight decay\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay)\n",
    "\n",
    "                # Momentum\n",
    "                if momentum != 0:\n",
    "                    buf = momentum_buffer_list[i]\n",
    "                    if buf is None:\n",
    "                        buf = d_p.clone().detach()\n",
    "                        momentum_buffer_list[i] = buf\n",
    "                    else:\n",
    "                        buf.mul_(momentum).add_(d_p, alpha=1 - group['dampening'] if 'dampening' in group else 1)\n",
    "                    d_p = buf\n",
    "\n",
    "                # Update step\n",
    "                p.add_(d_p, alpha=-lr)\n",
    "\n",
    "        return loss\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Part 2: Fisher Information (Sensitivity) Calculation \n",
    "# ---------------------------------------------------------\n",
    "def compute_fisher_sensitivity(model, dataloader, criterion, device, num_batches=10):\n",
    "    \"\"\"\n",
    "    Computes the diagonal Fisher Information Matrix (FIM) scores.\n",
    "    Sensitivity = Average of (Gradient ** 2)\n",
    "    \n",
    "    :param num_batches: Number of calibration rounds \n",
    "    :return: A dictionary {param: sensitivity_tensor}\n",
    "    \"\"\"\n",
    "    model.eval() # Gradients are still computed in eval mode if we don't use no_grad\n",
    "    sensitivity_scores = {}\n",
    "    \n",
    "    # Initialize accumulators\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            sensitivity_scores[p] = torch.zeros_like(p.data)\n",
    "\n",
    "    print(f\"Calculating sensitivity over {num_batches} batches...\")\n",
    "    \n",
    "    # Iterate over data\n",
    "    processed_batches = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        if processed_batches >= num_batches:\n",
    "            break\n",
    "            \n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass to get gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Accumulate squared gradients \n",
    "        # Diagonal Fisher Information ~ E[grad^2]\n",
    "        for p in model.parameters():\n",
    "            if p.requires_grad and p.grad is not None:\n",
    "                sensitivity_scores[p] += p.grad.data ** 2\n",
    "        \n",
    "        processed_batches += 1\n",
    "\n",
    "    # Normalize by number of batches\n",
    "    for p in sensitivity_scores:\n",
    "        sensitivity_scores[p] /= processed_batches\n",
    "        \n",
    "    return sensitivity_scores\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Part 3: Mask Calibration \n",
    "# ---------------------------------------------------------\n",
    "def calibrate_masks(sensitivity_scores, sparsity_ratio=0.1, keep_least_sensitive=True):\n",
    "    \"\"\"\n",
    "    Creates binary masks based on sensitivity scores.\n",
    "    \n",
    "    Task Arithmetic logic :\n",
    "    We usually want to modify \"low-sensitivity\" parameters to avoid \n",
    "    interfering with pre-trained knowledge.\n",
    "    \n",
    "    :param sparsity_ratio: Percentage of parameters to UPDATE (Mask=1).\n",
    "                           e.g., 0.1 means we update 10% of weights.\n",
    "    :param keep_least_sensitive: \n",
    "           If True: Update the LOWEST sensitivity weights (Mask=1 where Sens is Low).\n",
    "           If False: Update the HIGHEST sensitivity weights.\n",
    "    :return: A dictionary {param: binary_mask_tensor}\n",
    "    \"\"\"\n",
    "    masks = {}\n",
    "    \n",
    "    # We can compute the threshold globally or layer-wise. \n",
    "    # Global thresholding is common in Task Arithmetic literature.\n",
    "    \n",
    "    # 1. Flatten all scores to find the global threshold\n",
    "    all_scores = torch.cat([s.view(-1) for s in sensitivity_scores.values()])\n",
    "    \n",
    "    # 2. Determine threshold\n",
    "    # We want to select `sparsity_ratio` percent of parameters.\n",
    "    num_params = all_scores.numel()\n",
    "    k = int(num_params * sparsity_ratio)\n",
    "    \n",
    "    if keep_least_sensitive:\n",
    "        # We want to update the k LEAST sensitive parameters.\n",
    "        # So we look for the k-th smallest value.\n",
    "        # weights < threshold -> Mask 1 (Update)\n",
    "        # weights > threshold -> Mask 0 (Freeze)\n",
    "        threshold = torch.kthvalue(all_scores, k).values.item()\n",
    "        \n",
    "        for p, score in sensitivity_scores.items():\n",
    "            # Mask = 1 if score <= threshold (Low Sensitivity)\n",
    "            # Mask = 0 if score > threshold\n",
    "            mask = (score <= threshold).float()\n",
    "            masks[p] = mask\n",
    "    else:\n",
    "        # (For Task 4 Extension) Update MOST sensitive\n",
    "        # weights > threshold -> Mask 1\n",
    "        threshold = torch.kthvalue(all_scores, num_params - k).values.item()\n",
    "        \n",
    "        for p, score in sensitivity_scores.items():\n",
    "            mask = (score >= threshold).float()\n",
    "            masks[p] = mask\n",
    "            \n",
    "    return masks\n",
    "\n",
    "    def calibrate_masks_extended(model, strategy='least_sensitive', sparsity_ratio=0.1, sensitivity_scores=None):\n",
    "    \"\"\"\n",
    "    实现项目第4部分的扩展功能：多种梯度掩码校准规则 [cite: 77]\n",
    "    \"\"\"\n",
    "    masks = {}\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    # 1. 获取所有待评估的分数\n",
    "    if strategy in ['least_sensitive', 'most_sensitive']:\n",
    "        all_values = torch.cat([s.view(-1) for s in sensitivity_scores.values()])\n",
    "    elif strategy in ['low_magnitude', 'high_magnitude']:\n",
    "        all_values = torch.cat([p.data.abs().view(-1) for p in params])\n",
    "    elif strategy == 'random':\n",
    "        all_values = torch.cat([torch.rand_like(p).view(-1) for p in params])\n",
    "\n",
    "    # 2. 计算阈值\n",
    "    k = int(all_values.numel() * sparsity_ratio)\n",
    "    \n",
    "    if strategy in ['least_sensitive', 'low_magnitude']:\n",
    "        threshold = torch.kthvalue(all_values, k).values.item()\n",
    "    elif strategy in ['most_sensitive', 'high_magnitude']:\n",
    "        threshold = torch.kthvalue(all_values, all_values.numel() - k).values.item()\n",
    "    else: # random\n",
    "        threshold = torch.kthvalue(all_values, k).values.item()\n",
    "\n",
    "    # 3. 生成掩码\n",
    "    for p in params:\n",
    "        if strategy == 'least_sensitive':\n",
    "            masks[p] = (sensitivity_scores[p] <= threshold).float()\n",
    "        elif strategy == 'most_sensitive':\n",
    "            masks[p] = (sensitivity_scores[p] >= threshold).float()\n",
    "        elif strategy == 'low_magnitude':\n",
    "            masks[p] = (p.data.abs() <= threshold).float()\n",
    "        elif strategy == 'high_magnitude':\n",
    "            masks[p] = (p.data.abs() >= threshold).float()\n",
    "        elif strategy == 'random':\n",
    "            masks[p] = (torch.rand_like(p) <= (k/all_values.numel())).float()\n",
    "            \n",
    "    return masks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdvancedMachingLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
