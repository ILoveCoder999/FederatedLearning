{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ILoveCoder999/FederatedLearning/blob/master/dataPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NvJBelYjXwt7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvJBelYjXwt7",
        "outputId": "ff1050e0-3179-4732-d151-99d50b0367db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-wx0VcRwZw00",
      "metadata": {
        "id": "-wx0VcRwZw00"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Subset, random_split\n",
        "\n",
        "\n",
        "class FederatedDataBuilder:\n",
        "    def __init__(self, root='./data', val_split_ratio=0.1, K=100):\n",
        "        \n",
        "        self.root = root\n",
        "        self.K = K\n",
        "        self.val_split_ratio = val_split_ratio\n",
        "\n",
        "        # Image transformations for DINO ViT (requires 224x224)\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "\n",
        "        # Load CIFAR-100\n",
        "        full_train_dataset = torchvision.datasets.CIFAR100(\n",
        "            root=self.root, train=True, download=True, transform=self.transform\n",
        "        )\n",
        "        self.test_dataset = torchvision.datasets.CIFAR100(\n",
        "            root=self.root, train=False, download=True, transform=self.transform\n",
        "        )\n",
        "\n",
        "        # Create train-validation split\n",
        "        val_size = int(len(full_train_dataset) * val_split_ratio)\n",
        "        train_size = len(full_train_dataset) - val_size\n",
        "\n",
        "        self.train_dataset, self.val_dataset = random_split(\n",
        "            full_train_dataset, [train_size, val_size],\n",
        "            generator=torch.Generator().manual_seed(42)\n",
        "        )\n",
        "\n",
        "        # Extract labels for Non-IID partitioning\n",
        "        self.train_indices = self.train_dataset.indices\n",
        "        self.train_targets = np.array(full_train_dataset.targets)[self.train_indices]\n",
        "\n",
        "    def get_iid_partition(self):\n",
        "       \n",
        "        print(f\"Creating IID partition for {self.K} clients...\")\n",
        "        \n",
        "        num_items = int(len(self.train_dataset) / self.K)\n",
        "        dict_users = {}\n",
        "        all_idxs = list(range(len(self.train_dataset)))\n",
        "        np.random.shuffle(all_idxs)\n",
        "\n",
        "        for i in range(self.K):\n",
        "            dict_users[i] = set(all_idxs[i * num_items : (i + 1) * num_items])\n",
        "\n",
        "        return dict_users\n",
        "\n",
        "    def get_non_iid_partition(self, Nc):\n",
        "       \n",
        "        print(f\"Creating Non-IID partition (Nc={Nc}) for {self.K} clients...\")\n",
        "\n",
        "        # Sort by labels\n",
        "        idxs = np.arange(len(self.train_dataset))\n",
        "        labels = self.train_targets\n",
        "        idxs_labels = np.vstack((idxs, labels))\n",
        "        idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
        "        idxs = idxs_labels[0, :]\n",
        "\n",
        "        # Create shards\n",
        "        total_shards = self.K * Nc\n",
        "        shard_size = int(len(self.train_dataset) / total_shards)\n",
        "        idx_shard = [idxs[i*shard_size : (i+1)*shard_size] for i in range(total_shards)]\n",
        "\n",
        "        # Assign shards to clients\n",
        "        dict_users = {i: np.array([], dtype='int64') for i in range(self.K)}\n",
        "        available_shards = list(range(total_shards))\n",
        "\n",
        "        for i in range(self.K):\n",
        "            for _ in range(Nc):\n",
        "                shard_idx = np.random.choice(available_shards)\n",
        "                dict_users[i] = np.concatenate((dict_users[i], idx_shard[shard_idx]))\n",
        "                available_shards.remove(shard_idx)\n",
        "\n",
        "        return dict_users\n",
        "\n",
        "    def verify_partition(self, dict_users):\n",
        "      \n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Verifying Partition\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        # Check 1: Total samples\n",
        "        all_indices = set()\n",
        "        for client_indices in dict_users.values():\n",
        "            all_indices.update(client_indices)\n",
        "        \n",
        "        total = len(all_indices)\n",
        "        expected = len(self.train_dataset)\n",
        "        print(f\"Total samples: {total}/{expected}\")\n",
        "        \n",
        "        # Check 2: No overlap\n",
        "        sum_samples = sum(len(indices) for indices in dict_users.values())\n",
        "        if sum_samples != total:\n",
        "            print(f\"✗ Overlap detected!\")\n",
        "            return False\n",
        "        print(f\"No overlap\")\n",
        "        \n",
        "        # Check 3: Class distribution\n",
        "        classes_per_client = []\n",
        "        for client_indices in dict_users.values():\n",
        "            if isinstance(client_indices, set):\n",
        "                client_indices = list(client_indices)\n",
        "            client_labels = self.train_targets[client_indices]\n",
        "            num_classes = len(np.unique(client_labels))\n",
        "            classes_per_client.append(num_classes)\n",
        "        \n",
        "        avg_classes = np.mean(classes_per_client)\n",
        "        print(f\"✓ Avg classes per client: {avg_classes:.1f}\")\n",
        "        print(\"=\"*50 + \"\\n\")\n",
        "        \n",
        "        return total == expected\n",
        "\n",
        "    def visualize_distribution(self, dict_users, save_path=None, title=\"\"):\n",
        "        \n",
        "        num_clients = len(dict_users)\n",
        "        num_classes = 100\n",
        "        \n",
        "        # Build distribution matrix\n",
        "        matrix = np.zeros((num_clients, num_classes))\n",
        "        \n",
        "        for client_id, client_indices in dict_users.items():\n",
        "            if isinstance(client_indices, set):\n",
        "                client_indices = list(client_indices)\n",
        "            \n",
        "            client_labels = self.train_targets[client_indices]\n",
        "            unique_classes, counts = np.unique(client_labels, return_counts=True)\n",
        "            \n",
        "            for cls, count in zip(unique_classes, counts):\n",
        "                matrix[client_id, cls] = count\n",
        "        \n",
        "        # Plot\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.imshow(matrix, aspect='auto', cmap='YlOrRd', interpolation='nearest')\n",
        "        plt.colorbar(label='Number of Samples')\n",
        "        plt.xlabel('Class ID', fontsize=12)\n",
        "        plt.ylabel('Client ID', fontsize=12)\n",
        "        plt.title(f'Data Distribution - {title}', fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "            print(f\"Figure saved to {save_path}\")\n",
        "        \n",
        "        plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a65502",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7a65502",
        "outputId": "80b0e7bd-7ba7-4f34-df6d-77e382a270fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169M/169M [00:19<00:00, 8.65MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating I.I.D. partition for 100 clients...\n",
            "Client 0 IID sample count: 450\n",
            "Generating Non-I.I.D. partition for 100 clients with Nc=5...\n",
            "Client 0 Non-IID sample count: 450\n",
            "Client 0 has labels: [ 5 13 15 16 38 39]\n",
            "Number of unique classes for Client 0: 6\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize\n",
        "    data_builder = FederatedDataBuilder(K=100)\n",
        "    \n",
        "    # IID partition\n",
        "    dict_users_iid = data_builder.get_iid_partition()\n",
        "    data_builder.verify_partition(dict_users_iid)\n",
        "    data_builder.visualize_distribution(dict_users_iid, \n",
        "                                       save_path='iid.png',\n",
        "                                       title='IID')\n",
        "    \n",
        "    # Non-IID partition\n",
        "    dict_users_nc5 = data_builder.get_non_iid_partition(Nc=5)\n",
        "    data_builder.verify_partition(dict_users_nc5)\n",
        "    data_builder.visualize_distribution(dict_users_nc5,\n",
        "                                       save_path='non_iid_nc5.png', \n",
        "                                       title='Non-IID (Nc=5)')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
