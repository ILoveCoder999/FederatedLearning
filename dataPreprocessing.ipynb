{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ILoveCoder999/FederatedLearning/blob/master/dataPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvJBelYjXwt7",
        "outputId": "478d47c1-384f-4f7f-b74d-5c54555ad69a"
      },
      "id": "NvJBelYjXwt7",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset, random_split\n",
        "\n",
        "class FederatedDataBuilder:\n",
        "    def __init__(self, root='./data', val_split_ratio=0.1, K=100):\n",
        "        \"\"\"\n",
        "        Initialize the data builder.\n",
        "        :param root: Directory to download/store the dataset.\n",
        "        :param val_split_ratio: Ratio of the validation set size to the training set size.\n",
        "        :param K: Total number of clients (Source 59, 62 typically sets K=100).\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.K = K\n",
        "        self.val_split_ratio = val_split_ratio\n",
        "\n",
        "        # 1. Data Preprocessing\n",
        "        # Note: DINO ViT might require specific transforms (e.g., resize to 224x224) depending on memory.\n",
        "        # Using standard CIFAR transforms here as a baseline.\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "\n",
        "        # 2. Load original CIFAR-100 training set\n",
        "        # The project requires CIFAR-100.\n",
        "        full_train_dataset = torchvision.datasets.CIFAR100(\n",
        "            root=self.root, train=True, download=True, transform=self.transform\n",
        "        )\n",
        "        self.test_dataset = torchvision.datasets.CIFAR100(\n",
        "            root=self.root, train=False, download=True, transform=self.transform\n",
        "        )\n",
        "\n",
        "        # 3. Create Validation Split\n",
        "        # CIFAR-100 does not have a validation split, so we must create one.\n",
        "        val_size = int(len(full_train_dataset) * val_split_ratio) #val_size=5000\n",
        "        train_size = len(full_train_dataset) - val_size  #train_size=45000\n",
        "\n",
        "        # This 'train_dataset' will be used for subsequent FL client partitioning.\n",
        "        # A fixed seed is used for reproducibility.\n",
        "        self.train_dataset, self.val_dataset = random_split(\n",
        "            full_train_dataset, [train_size, val_size],\n",
        "            generator=torch.Generator().manual_seed(42)\n",
        "        )\n",
        "\n",
        "        # Extract targets/labels for the training subset to handle Non-IID logic.\n",
        "        # Note: We need to map the subset indices back to the original dataset targets.\n",
        "        self.train_indices = self.train_dataset.indices\n",
        "        self.train_targets = np.array(full_train_dataset.targets)[self.train_indices]\n",
        "\n",
        "    def get_iid_partition(self):\n",
        "        \"\"\"\n",
        "        I.I.D. Sharding: Each client is given an approximately equal number of training\n",
        "        samples uniformly distributed over the class labels .\n",
        "        \"\"\"\n",
        "        print(f\"Generating I.I.D. partition for {self.K} clients...\")\n",
        "        num_items = int(len(self.train_dataset) / self.K)\n",
        "        dict_users, all_idxs = {}, [i for i in range(len(self.train_dataset))]\n",
        "\n",
        "        # Randomly shuffle all indices to ensure uniform distribution\n",
        "        np.random.shuffle(all_idxs)\n",
        "\n",
        "        for i in range(self.K):\n",
        "            # Assign a slice of indices to each client\n",
        "            dict_users[i] = set(all_idxs[i * num_items : (i + 1) * num_items])\n",
        "\n",
        "        return dict_users\n",
        "\n",
        "    def get_non_iid_partition(self, Nc):\n",
        "        \"\"\"\n",
        "        Non-I.I.D. Sharding: Each client is given an approximately equal number of training\n",
        "        samples, belonging to Nc classes .\n",
        "\n",
        "        :param Nc: The number of classes per client (Controls heterogeneity).\n",
        "        \"\"\"\n",
        "        print(f\"Generating Non-I.I.D. partition for {self.K} clients with Nc={Nc}...\")\n",
        "\n",
        "        # 1. Sort indices by label to group classes together\n",
        "        idxs = np.arange(len(self.train_dataset))\n",
        "        labels = self.train_targets\n",
        "\n",
        "        # Stack indices and labels, then sort by labels\n",
        "        idxs_labels = np.vstack((idxs, labels))\n",
        "        idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
        "        idxs = idxs_labels[0, :] # Sorted indices\n",
        "        labels = idxs_labels[1, :] # Sorted labels\n",
        "\n",
        "        # 2. Create Shards\n",
        "        # To ensure each client gets exactly Nc classes (approximately), we divide the\n",
        "        # sorted dataset into K * Nc shards.\n",
        "        total_shards = self.K * Nc\n",
        "        shard_size = int(len(self.train_dataset) / total_shards)\n",
        "\n",
        "        # Split the sorted indices into shards\n",
        "        idx_shard = [idxs[i*shard_size : (i+1)*shard_size] for i in range(total_shards)]\n",
        "\n",
        "        # 3. Assign Shards to Clients\n",
        "        # Clients must have disjoint sets of training samples.\n",
        "        dict_users = {i: np.array([], dtype='int64') for i in range(self.K)}\n",
        "        available_shards = list(range(total_shards))\n",
        "\n",
        "        for i in range(self.K):\n",
        "            # Assign Nc shards to each client\n",
        "            shards_to_assign = []\n",
        "            for _ in range(Nc):\n",
        "                # Randomly select a shard and remove it from the pool (no replacement)\n",
        "                # to ensure the data subsets are disjoint.\n",
        "                shard_idx = np.random.choice(available_shards)\n",
        "                shards_to_assign.append(shard_idx)\n",
        "                available_shards.remove(shard_idx)\n",
        "\n",
        "            # Concatenate the selected shards for this client\n",
        "            for shard_idx in shards_to_assign:\n",
        "                dict_users[i] = np.concatenate((dict_users[i], idx_shard[shard_idx]), axis=0)\n",
        "\n",
        "        return dict_users\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-wx0VcRwZw00"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-wx0VcRwZw00"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c7a65502",
      "metadata": {
        "id": "c7a65502",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2009ad8-0b94-444d-c262-11f863ecd6d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/preprocessing.py\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # 1. Initialize Builder\n",
        "    # Note: First run will download CIFAR-100 to ./data\n",
        "    builder = FederatedDataBuilder(val_split_ratio=0.1, K=100)\n",
        "\n",
        "    # 2. Get I.I.D. partition\n",
        "    iid_dict = builder.get_iid_partition()\n",
        "    print(f\"Client 0 IID sample count: {len(iid_dict[0])}\")\n",
        "\n",
        "    # 3. Get Non-I.I.D. partition (e.g., Nc=5)\n",
        "    # You need to test with Nc={1, 5, 10, 50}\n",
        "    non_iid_dict = builder.get_non_iid_partition(Nc=5)\n",
        "    print(f\"Client 0 Non-IID sample count: {len(non_iid_dict[0])}\")\n",
        "\n",
        "    # 4. Verification\n",
        "    # Check if Client 0 actually has samples from approximately Nc classes\n",
        "    client_0_indices = list(non_iid_dict[0])\n",
        "    c0_labels = builder.train_targets[client_0_indices]\n",
        "    unique_labels = np.unique(c0_labels)\n",
        "\n",
        "    print(f\"Client 0 has labels: {unique_labels}\")\n",
        "    print(f\"Number of unique classes for Client 0: {len(unique_labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/preprocessing.py\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset, random_split\n",
        "\n",
        "class FederatedDataBuilder:\n",
        "    def __init__(self, root='./data', val_split_ratio=0.1, K=100):\n",
        "        \"\"\"\n",
        "        Initialize the data builder.\n",
        "        :param root: Directory to download/store the dataset.\n",
        "        :param val_split_ratio: Ratio of the validation set size to the training set size.\n",
        "        :param K: Total number of clients (Source 59, 62 typically sets K=100).\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.K = K\n",
        "        self.val_split_ratio = val_split_ratio\n",
        "\n",
        "        # 1. Data Preprocessing\n",
        "        # Note: DINO ViT might require specific transforms (e.g., resize to 224x224) depending on memory.\n",
        "        # Using standard CIFAR transforms here as a baseline.\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "\n",
        "        # 2. Load original CIFAR-100 training set\n",
        "        # The project requires CIFAR-100.\n",
        "        full_train_dataset = torchvision.datasets.CIFAR100(\n",
        "            root=self.root, train=True, download=True, transform=self.transform\n",
        "        )\n",
        "        self.test_dataset = torchvision.datasets.CIFAR100(\n",
        "            root=self.root, train=False, download=True, transform=self.transform\n",
        "        )\n",
        "\n",
        "        # 3. Create Validation Split\n",
        "        # CIFAR-100 does not have a validation split, so we must create one.\n",
        "        val_size = int(len(full_train_dataset) * val_split_ratio) #val_size=5000\n",
        "        train_size = len(full_train_dataset) - val_size  #train_size=45000\n",
        "\n",
        "        # This 'train_dataset' will be used for subsequent FL client partitioning.\n",
        "        # A fixed seed is used for reproducibility.\n",
        "        self.train_dataset, self.val_dataset = random_split(\n",
        "            full_train_dataset, [train_size, val_size],\n",
        "            generator=torch.Generator().manual_seed(42)\n",
        "        )\n",
        "\n",
        "        # Extract targets/labels for the training subset to handle Non-IID logic.\n",
        "        # Note: We need to map the subset indices back to the original dataset targets.\n",
        "        self.train_indices = self.train_dataset.indices\n",
        "        self.train_targets = np.array(full_train_dataset.targets)[self.train_indices]\n",
        "\n",
        "    def get_iid_partition(self):\n",
        "        \"\"\"\n",
        "        I.I.D. Sharding: Each client is given an approximately equal number of training\n",
        "        samples uniformly distributed over the class labels .\n",
        "        \"\"\"\n",
        "        print(f\"Generating I.I.D. partition for {self.K} clients...\")\n",
        "        num_items = int(len(self.train_dataset) / self.K)\n",
        "        dict_users, all_idxs = {}, [i for i in range(len(self.train_dataset))]\n",
        "\n",
        "        # Randomly shuffle all indices to ensure uniform distribution\n",
        "        np.random.shuffle(all_idxs)\n",
        "\n",
        "        for i in range(self.K):\n",
        "            # Assign a slice of indices to each client\n",
        "            dict_users[i] = set(all_idxs[i * num_items : (i + 1) * num_items])\n",
        "\n",
        "        return dict_users\n",
        "\n",
        "    def get_non_iid_partition(self, Nc):\n",
        "        \"\"\"\n",
        "        Non-I.I.D. Sharding: Each client is given an approximately equal number of training\n",
        "        samples, belonging to Nc classes .\n",
        "\n",
        "        :param Nc: The number of classes per client (Controls heterogeneity).\n",
        "        \"\"\"\n",
        "        print(f\"Generating Non-I.I.D. partition for {self.K} clients with Nc={Nc}...\")\n",
        "\n",
        "        # 1. Sort indices by label to group classes together\n",
        "        idxs = np.arange(len(self.train_dataset))\n",
        "        labels = self.train_targets\n",
        "\n",
        "        # Stack indices and labels, then sort by labels\n",
        "        idxs_labels = np.vstack((idxs, labels))\n",
        "        idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
        "        idxs = idxs_labels[0, :] # Sorted indices\n",
        "        labels = idxs_labels[1, :] # Sorted labels\n",
        "\n",
        "        # 2. Create Shards\n",
        "        # To ensure each client gets exactly Nc classes (approximately), we divide the\n",
        "        # sorted dataset into K * Nc shards.\n",
        "        total_shards = self.K * Nc\n",
        "        shard_size = int(len(self.train_dataset) / total_shards)\n",
        "\n",
        "        # Split the sorted indices into shards\n",
        "        idx_shard = [idxs[i*shard_size : (i+1)*shard_size] for i in range(total_shards)]\n",
        "\n",
        "        # 3. Assign Shards to Clients\n",
        "        # Clients must have disjoint sets of training samples.\n",
        "        dict_users = {i: np.array([], dtype='int64') for i in range(self.K)}\n",
        "        available_shards = list(range(total_shards))\n",
        "\n",
        "        for i in range(self.K):\n",
        "            # Assign Nc shards to each client\n",
        "            shards_to_assign = []\n",
        "            for _ in range(Nc):\n",
        "                # Randomly select a shard and remove it from the pool (no replacement)\n",
        "                # to ensure the data subsets are disjoint.\n",
        "                shard_idx = np.random.choice(available_shards)\n",
        "                shards_to_assign.append(shard_idx)\n",
        "                available_shards.remove(shard_idx)\n",
        "\n",
        "            # Concatenate the selected shards for this client\n",
        "            for shard_idx in shards_to_assign:\n",
        "                dict_users[i] = np.concatenate((dict_users[i], idx_shard[shard_idx]), axis=0)\n",
        "\n",
        "        return dict_users\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i5Y0INkGZVHA"
      },
      "id": "i5Y0INkGZVHA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}