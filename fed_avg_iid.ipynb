{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ILoveCoder999/FederatedLearning/blob/master/fed_avg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "RwWZRs5ju7iw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwWZRs5ju7iw",
        "outputId": "403965c7-7aa8-42f2-9b92-dd9033ce68e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# import module\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive')\n",
        "from preprocessing import FederatedDataBuilder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d4d9643",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3d4d9643",
        "outputId": "86bd0908-5b6f-444b-e4a9-d30b4a17d9a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing fed_avg_iid.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile fed_avg_iid.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "from preprocessing import FederatedDataBuilder\n",
        "\n",
        "\n",
        "GLOBAL_DINO_BACKBONE = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
        "\n",
        "\n",
        "\n",
        "class DINOCIFAR100Fixed(nn.Module):\n",
        "    \"\"\"\n",
        "     Uses global backbone instead of reloading each time\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=100):\n",
        "        super().__init__()\n",
        "\n",
        "        # Use the global backbone (not loading a new one!)\n",
        "        self.backbone = GLOBAL_DINO_BACKBONE\n",
        "\n",
        "        # Freeze backbone\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Classification head (this is what we train)\n",
        "        self.head = nn.Linear(384, num_classes)\n",
        "\n",
        "        # Initialize head properly\n",
        "        nn.init.xavier_uniform_(self.head.weight)\n",
        "        nn.init.zeros_(self.head.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features\n",
        "        with torch.no_grad():\n",
        "            features = self.backbone(x)\n",
        "\n",
        "        # Classify\n",
        "        output = self.head(features)\n",
        "        return output\n",
        "\n",
        "\n",
        "def fed_avg_aggregate(global_model, local_weights, client_sample_counts):\n",
        "    \"\"\"Weighted averaging of local model weights\"\"\"\n",
        "    global_dict = copy.deepcopy(global_model.state_dict())\n",
        "    total_samples = sum(client_sample_counts)\n",
        "\n",
        "    # Initialize to zero\n",
        "    for k in global_dict.keys():\n",
        "        if 'num_batches_tracked' not in k and 'backbone' not in k:\n",
        "            global_dict[k] = global_dict[k] * 0.0\n",
        "\n",
        "    # Weighted average\n",
        "    for i in range(len(local_weights)):\n",
        "        ratio = client_sample_counts[i] / total_samples\n",
        "        weights = local_weights[i]\n",
        "        for k in global_dict.keys():\n",
        "            if 'num_batches_tracked' not in k and 'backbone' not in k:\n",
        "                global_dict[k] += weights[k] * ratio\n",
        "\n",
        "    return global_dict\n",
        "\n",
        "\n",
        "class LocalClient:\n",
        "    \"\"\"Local client that trains ONLY the classification head\"\"\"\n",
        "\n",
        "    def __init__(self, client_id, dataset, indices, device):\n",
        "        self.client_id = client_id\n",
        "        self.indices = indices\n",
        "        self.device = device\n",
        "\n",
        "        self.trainloader = DataLoader(\n",
        "            Subset(dataset, list(indices)),\n",
        "            batch_size=128,\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    def train(self, global_weights, local_steps=4, lr=0.1):\n",
        "        \"\"\"Train for J=4 steps\"\"\"\n",
        "\n",
        "        # Create model (uses global backbone, NO reloading!)\n",
        "        local_model = DINOCIFAR100Fixed(num_classes=100).to(self.device)\n",
        "\n",
        "        # Load global weights\n",
        "        local_model.load_state_dict(global_weights, strict=False)\n",
        "        local_model.train()\n",
        "\n",
        "        # Only train the head, backbone stays frozen\n",
        "        optimizer = optim.SGD(\n",
        "            local_model.head.parameters(),  # ONLY head parameters!\n",
        "            lr=lr,\n",
        "            momentum=0.9,\n",
        "            weight_decay=1e-4,\n",
        "            nesterov=True\n",
        "        )\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Train for J steps\n",
        "        step_count = 0\n",
        "        losses = []\n",
        "        iterator = iter(self.trainloader)\n",
        "\n",
        "        while step_count < local_steps:\n",
        "            try:\n",
        "                inputs, targets = next(iterator)\n",
        "            except StopIteration:\n",
        "                iterator = iter(self.trainloader)\n",
        "                inputs, targets = next(iterator)\n",
        "\n",
        "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = local_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(local_model.head.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            step_count += 1\n",
        "\n",
        "        return local_model.state_dict(), sum(losses)/len(losses)\n",
        "\n",
        "\n",
        "def evaluate_global(model, test_loader, device):\n",
        "    \"\"\"Evaluate global model\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_sum = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss_sum += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return loss_sum / len(test_loader), 100. * correct / total\n",
        "\n",
        "\n",
        "def run_fedavg_fixed(K=100, C=0.1, J=8, rounds=100, lr=0.05):\n",
        "    \"\"\"\n",
        "    FedAvg with backbone reloading issue FIXED\n",
        "    \"\"\"\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"FedAvg with FIXED Backbone Loading\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"K={K}, C={C}, J={J}, Rounds={rounds}\")\n",
        "    print(f\"Learning rate: {lr}\")\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Data\n",
        "    print(\"Preparing data...\")\n",
        "    data_builder = FederatedDataBuilder(val_split_ratio=0.1, K=K)\n",
        "    client_dict = data_builder.get_iid_partition()\n",
        "    data_builder.verify_partition(client_dict)\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        data_builder.test_dataset,\n",
        "        batch_size=256,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Global model\n",
        "    print(\"\\nInitializing global model...\")\n",
        "    global_model = DINOCIFAR100Fixed(num_classes=100).to(DEVICE)\n",
        "    print(\"✓ Global model ready (using shared backbone)\")\n",
        "\n",
        "    history = {'loss': [], 'accuracy': [], 'round': []}\n",
        "\n",
        "    # Training\n",
        "    m = max(int(C * K), 1)\n",
        "    print(f\"\\nStarting training with {m} clients per round...\\n\")\n",
        "\n",
        "    for r in range(rounds):\n",
        "        selected_clients = np.random.choice(range(K), m, replace=False)\n",
        "\n",
        "        local_weights = []\n",
        "        client_sample_counts = []\n",
        "        client_losses = []\n",
        "\n",
        "        global_weights = copy.deepcopy(global_model.state_dict())\n",
        "\n",
        "        # Train clients\n",
        "        for idx, client_idx in enumerate(selected_clients):\n",
        "            client = LocalClient(\n",
        "                client_id=client_idx,\n",
        "                dataset=data_builder.train_dataset,\n",
        "                indices=client_dict[client_idx],\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "            # NO backbone reloading here!\n",
        "            w_local, loss_local = client.train(global_weights, local_steps=J, lr=lr)\n",
        "\n",
        "            local_weights.append(w_local)\n",
        "            client_sample_counts.append(len(client_dict[client_idx]))\n",
        "            client_losses.append(loss_local)\n",
        "\n",
        "        # Aggregate\n",
        "        new_weights = fed_avg_aggregate(global_model, local_weights, client_sample_counts)\n",
        "        global_model.load_state_dict(new_weights, strict=False)\n",
        "\n",
        "        # Evaluate\n",
        "        test_loss, test_acc = evaluate_global(global_model, test_loader, DEVICE)\n",
        "        history['loss'].append(test_loss)\n",
        "        history['accuracy'].append(test_acc)\n",
        "        history['round'].append(r + 1)\n",
        "\n",
        "        avg_client_loss = sum(client_losses) / len(client_losses)\n",
        "\n",
        "        print(f\"Round {r+1:2d}/{rounds} -> \"\n",
        "              f\"Train Loss: {avg_client_loss:.4f} | \"\n",
        "              f\"Test Loss: {test_loss:.4f} | \"\n",
        "              f\"Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "    # Results\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training Complete!\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Final Test Accuracy: {history['accuracy'][-1]:.2f}%\")\n",
        "    print(f\"Best Test Accuracy: {max(history['accuracy']):.2f}%\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['round'], history['accuracy'], 'b-o', linewidth=2)\n",
        "    plt.title('Test Accuracy (Fixed Version)', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['round'], history['loss'], 'r-o', linewidth=2)\n",
        "    plt.title('Test Loss', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('fedavg_backbone_fixed.png', dpi=150)\n",
        "    print(\"Figure saved: fedavg_backbone_fixed.png\")\n",
        "    plt.show()\n",
        "\n",
        "    return history, global_model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    history, model = run_fedavg_fixed(\n",
        "        K=100,\n",
        "        C=0.1,\n",
        "        J=4,\n",
        "        rounds=200,\n",
        "        lr=0.05\n",
        "    )\n",
        "\n",
        "    print(\"\\n✓ Training completed!\")\n",
        "    print(\"\\nComparison:\")\n",
        "    print(\"  Before fix: ~5% after 21 rounds\")\n",
        "    print(f\"  After fix:  {history['accuracy'][-1]:.2f}% after 30 rounds\")\n",
        "    print(\"\\nThis should be a significant improvement!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AdvancedMachingLearning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
