{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170ae2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Loading DINO backbone (ONE TIME ONLY)...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/dino/zipball/main\" to /Users/van/.cache/torch/hub/main.zip\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\" to /Users/van/.cache/torch/hub/checkpoints/dino_deitsmall16_pretrain.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DINO backbone loaded and cached globally\n",
      "======================================================================\n",
      "Creating data builder (will be reused for all experiments)...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 1: IID Baseline (J=4)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "FedAvg Experiment: IID, J=4\n",
      "======================================================================\n",
      "K=100, C=0.1, J=4, Rounds=50, LR=0.1\n",
      "Device: cpu\n",
      "======================================================================\n",
      "\n",
      "Creating IID partition for 100 clients...\n",
      "Initializing global model...\n",
      "Training with 10 clients per round\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 553\u001b[0m\n\u001b[1;32m    546\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    550\u001b[0m    \n\u001b[1;32m    551\u001b[0m     \n\u001b[1;32m    552\u001b[0m     \u001b[38;5;66;03m# Run all experiments\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m     all_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_comprehensive_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ All experiments completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerated plots:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 427\u001b[0m, in \u001b[0;36mrun_comprehensive_experiments\u001b[0;34m()\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m    426\u001b[0m rounds_j4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m--> 427\u001b[0m history_iid_j4, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fedavg_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mJ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrounds_j4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# IID\u001b[39;49;00m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_builder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_builder\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m all_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIID_J4\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m history_iid_j4\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# ================================================================\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# Experiment 2: Non-IID with varying Nc and J\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# ================================================================\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 362\u001b[0m, in \u001b[0;36mrun_fedavg_experiment\u001b[0;34m(K, C, J, rounds, lr, Nc, data_builder)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    355\u001b[0m client \u001b[38;5;241m=\u001b[39m LocalClient(\n\u001b[1;32m    356\u001b[0m     client_id\u001b[38;5;241m=\u001b[39mclient_idx,\n\u001b[1;32m    357\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdata_builder\u001b[38;5;241m.\u001b[39mtrain_dataset,\n\u001b[1;32m    358\u001b[0m     indices\u001b[38;5;241m=\u001b[39mclient_dict[client_idx],\n\u001b[1;32m    359\u001b[0m     device\u001b[38;5;241m=\u001b[39mDEVICE\n\u001b[1;32m    360\u001b[0m )\n\u001b[0;32m--> 362\u001b[0m w_local, loss_local \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mJ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m local_weights\u001b[38;5;241m.\u001b[39mappend(w_local)\n\u001b[1;32m    365\u001b[0m client_sample_counts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(client_dict[client_idx]))\n",
      "Cell \u001b[0;32mIn[2], line 242\u001b[0m, in \u001b[0;36mLocalClient.train\u001b[0;34m(self, global_weights, local_steps, lr)\u001b[0m\n\u001b[1;32m    239\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), targets\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    241\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 242\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m    244\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AdvancedMachingLearning/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AdvancedMachingLearning/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 39\u001b[0m, in \u001b[0;36mDINOCIFAR100.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 39\u001b[0m         features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(features)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AdvancedMachingLearning/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AdvancedMachingLearning/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_dino_main/vision_transformer.py:212\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    210\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_tokens(x)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 212\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AdvancedMachingLearning/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AdvancedMachingLearning/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_dino_main/vision_transformer.py:108\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, return_attention)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, return_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 108\u001b[0m     y, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_attention:\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m attn\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AdvancedMachingLearning/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AdvancedMachingLearning/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_dino_main/vision_transformer.py:91\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, N, C)\n\u001b[1;32m     90\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x)\n\u001b[0;32m---> 91\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj_drop\u001b[49m(x)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, attn\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AdvancedMachingLearning/lib/python3.9/site-packages/torch/nn/modules/module.py:1675\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1677\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FedAvg with Non-IID Data Distribution\n",
    "======================================\n",
    "Implements experiments required by the project:\n",
    "- Nc ∈ {1, 5, 10, 50} (classes per client)\n",
    "- J ∈ {4, 8, 16} (local steps)\n",
    "- K=100, C=0.1 (100 clients, 10% sampling)\n",
    "\n",
    "Based on project specification page 3-4.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "from preprocessing import FederatedDataBuilder\n",
    "\n",
    "# Load DINO backbone once globally\n",
    "print(\"=\"*70)\n",
    "print(\"Loading DINO backbone (ONE TIME ONLY)...\")\n",
    "print(\"=\"*70)\n",
    "GLOBAL_DINO_BACKBONE = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "print(\"✓ DINO backbone loaded and cached globally\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "class DINOCIFAR100(nn.Module):\n",
    "    \"\"\"DINO ViT-S/16 for CIFAR-100\"\"\"\n",
    "    def __init__(self, num_classes=100):\n",
    "        super().__init__()\n",
    "        self.backbone = GLOBAL_DINO_BACKBONE\n",
    "        \n",
    "        # Freeze backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Trainable classification head\n",
    "        self.head = nn.Linear(384, num_classes)\n",
    "        nn.init.xavier_uniform_(self.head.weight)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.backbone(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "def create_non_iid_partition(dataset, K, Nc, seed=42):\n",
    "    \"\"\"\n",
    "    Create Non-IID data partition.\n",
    "    \n",
    "    Args:\n",
    "        dataset: CIFAR-100 training dataset\n",
    "        K: Number of clients\n",
    "        Nc: Number of classes per client\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        client_dict: {client_id: [sample_indices]}\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    num_classes = 100\n",
    "    \n",
    "    # Group samples by class\n",
    "    class_to_indices = defaultdict(list)\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        class_to_indices[label].append(idx)\n",
    "    \n",
    "    # Shuffle indices within each class\n",
    "    for label in class_to_indices:\n",
    "        np.random.shuffle(class_to_indices[label])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Creating Non-IID Partition (Nc={Nc})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if Nc == 1:\n",
    "        # Each client gets one class\n",
    "        # Need to handle the case where K > num_classes\n",
    "        client_dict = {}\n",
    "        \n",
    "        for client_id in range(K):\n",
    "            # Assign class in round-robin fashion\n",
    "            assigned_class = client_id % num_classes\n",
    "            \n",
    "            # Calculate how many samples this client should get\n",
    "            samples_per_client = len(class_to_indices[assigned_class]) // (K // num_classes + 1)\n",
    "            \n",
    "            # Get indices for this client\n",
    "            start_idx = (client_id // num_classes) * samples_per_client\n",
    "            end_idx = start_idx + samples_per_client\n",
    "            \n",
    "            client_dict[client_id] = class_to_indices[assigned_class][start_idx:end_idx]\n",
    "        \n",
    "        print(f\"Distribution: Each client has 1 class\")\n",
    "        print(f\"Example: Client 0 has class {0 % num_classes}\")\n",
    "        \n",
    "    else:\n",
    "        # Each client gets Nc classes\n",
    "        client_dict = {}\n",
    "        \n",
    "        # Assign classes to clients\n",
    "        # Strategy: Randomly assign Nc classes to each client\n",
    "        all_classes = list(range(num_classes))\n",
    "        \n",
    "        for client_id in range(K):\n",
    "            # Randomly select Nc classes for this client\n",
    "            np.random.seed(seed + client_id)  # Deterministic per client\n",
    "            assigned_classes = np.random.choice(all_classes, Nc, replace=False)\n",
    "            \n",
    "            # Collect all samples from assigned classes\n",
    "            client_indices = []\n",
    "            for cls in assigned_classes:\n",
    "                # Divide samples of this class among clients that have it\n",
    "                # For simplicity, give each client a portion\n",
    "                samples_in_class = class_to_indices[cls]\n",
    "                portion = len(samples_in_class) // K  # Rough estimate\n",
    "                \n",
    "                start = (client_id * portion) % len(samples_in_class)\n",
    "                end = start + max(1, portion)\n",
    "                \n",
    "                client_indices.extend(samples_in_class[start:end])\n",
    "            \n",
    "            client_dict[client_id] = client_indices\n",
    "            \n",
    "            if client_id == 0:\n",
    "                print(f\"Example: Client 0 assigned classes: {sorted(assigned_classes)}\")\n",
    "    \n",
    "    # Verify\n",
    "    total_samples = sum(len(indices) for indices in client_dict.values())\n",
    "    avg_samples = total_samples / K\n",
    "    \n",
    "    print(f\"Total samples distributed: {total_samples}/{len(dataset)}\")\n",
    "    print(f\"Avg samples per client: {avg_samples:.1f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return client_dict\n",
    "\n",
    "\n",
    "def verify_non_iid_partition(dataset, client_dict, Nc):\n",
    "    \"\"\"Verify the Non-IID partition quality\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Verifying Non-IID Partition (Nc={Nc})\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Check number of classes per client\n",
    "    client_class_counts = []\n",
    "    \n",
    "    for client_id, indices in client_dict.items():\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get unique classes in this client\n",
    "        labels = [dataset[idx][1] for idx in indices]\n",
    "        unique_classes = len(set(labels))\n",
    "        client_class_counts.append(unique_classes)\n",
    "    \n",
    "    avg_classes = np.mean(client_class_counts)\n",
    "    std_classes = np.std(client_class_counts)\n",
    "    \n",
    "    print(f\"Classes per client:\")\n",
    "    print(f\"  Expected: {Nc}\")\n",
    "    print(f\"  Actual avg: {avg_classes:.2f} ± {std_classes:.2f}\")\n",
    "    print(f\"  Min: {min(client_class_counts)}, Max: {max(client_class_counts)}\")\n",
    "    \n",
    "    # Check sample distribution\n",
    "    sample_counts = [len(indices) for indices in client_dict.values()]\n",
    "    print(f\"\\nSamples per client:\")\n",
    "    print(f\"  Avg: {np.mean(sample_counts):.1f} ± {np.std(sample_counts):.1f}\")\n",
    "    print(f\"  Min: {min(sample_counts)}, Max: {max(sample_counts)}\")\n",
    "    \n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "\n",
    "def fed_avg_aggregate(global_model, local_weights, client_sample_counts):\n",
    "    \"\"\"FedAvg aggregation\"\"\"\n",
    "    global_dict = copy.deepcopy(global_model.state_dict())\n",
    "    total_samples = sum(client_sample_counts)\n",
    "    \n",
    "    # Initialize to zero\n",
    "    for k in global_dict.keys():\n",
    "        if 'num_batches_tracked' not in k and 'backbone' not in k:\n",
    "            global_dict[k] = global_dict[k] * 0.0\n",
    "    \n",
    "    # Weighted average\n",
    "    for i in range(len(local_weights)):\n",
    "        ratio = client_sample_counts[i] / total_samples\n",
    "        weights = local_weights[i]\n",
    "        for k in global_dict.keys():\n",
    "            if 'num_batches_tracked' not in k and 'backbone' not in k:\n",
    "                global_dict[k] += weights[k] * ratio\n",
    "    \n",
    "    return global_dict\n",
    "\n",
    "\n",
    "class LocalClient:\n",
    "    \"\"\"Local client for federated training\"\"\"\n",
    "    \n",
    "    def __init__(self, client_id, dataset, indices, device):\n",
    "        self.client_id = client_id\n",
    "        self.indices = indices\n",
    "        self.device = device\n",
    "        \n",
    "        self.trainloader = DataLoader(\n",
    "            Subset(dataset, list(indices)),\n",
    "            batch_size=64,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def train(self, global_weights, local_steps, lr=0.1):\n",
    "        \"\"\"Train for J local steps\"\"\"\n",
    "        \n",
    "        # Create local model\n",
    "        local_model = DINOCIFAR100(num_classes=100).to(self.device)\n",
    "        local_model.load_state_dict(global_weights, strict=False)\n",
    "        local_model.train()\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = optim.SGD(\n",
    "            local_model.head.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Train for J steps\n",
    "        step_count = 0\n",
    "        losses = []\n",
    "        iterator = iter(self.trainloader)\n",
    "        \n",
    "        while step_count < local_steps:\n",
    "            try:\n",
    "                inputs, targets = next(iterator)\n",
    "            except StopIteration:\n",
    "                iterator = iter(self.trainloader)\n",
    "                inputs, targets = next(iterator)\n",
    "            \n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = local_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(local_model.head.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            step_count += 1\n",
    "        \n",
    "        return local_model.state_dict(), sum(losses)/len(losses)\n",
    "\n",
    "\n",
    "def evaluate_global(model, test_loader, device):\n",
    "    \"\"\"Evaluate global model\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss_sum += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return loss_sum / len(test_loader), 100. * correct / total\n",
    "\n",
    "\n",
    "def run_fedavg_experiment(K=100, C=0.1, J=4, rounds=50, lr=0.1, \n",
    "                         Nc=None, data_builder=None):\n",
    "    \"\"\"\n",
    "    Run FedAvg experiment.\n",
    "    \n",
    "    Args:\n",
    "        K: Number of clients\n",
    "        C: Client sampling fraction\n",
    "        J: Local steps\n",
    "        rounds: Communication rounds\n",
    "        lr: Learning rate\n",
    "        Nc: Number of classes per client (None for IID)\n",
    "        data_builder: Pre-created data builder (to reuse data split)\n",
    "    \"\"\"\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    dist_type = f\"Non-IID (Nc={Nc})\" if Nc else \"IID\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FedAvg Experiment: {dist_type}, J={J}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"K={K}, C={C}, J={J}, Rounds={rounds}, LR={lr}\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Data preparation\n",
    "    if data_builder is None:\n",
    "        data_builder = FederatedDataBuilder(val_split_ratio=0.1, K=K)\n",
    "    \n",
    "    # Get appropriate partition\n",
    "    if Nc is None:\n",
    "        # IID\n",
    "        client_dict = data_builder.get_iid_partition()\n",
    "    else:\n",
    "        # Non-IID\n",
    "        client_dict = create_non_iid_partition(\n",
    "            data_builder.train_dataset, K, Nc\n",
    "        )\n",
    "        verify_non_iid_partition(data_builder.train_dataset, client_dict, Nc)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        data_builder.test_dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Global model\n",
    "    print(\"Initializing global model...\")\n",
    "    global_model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
    "    \n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'accuracy': [],\n",
    "        'round': [],\n",
    "        'train_loss': []\n",
    "    }\n",
    "    \n",
    "    # Training\n",
    "    m = max(int(C * K), 1)\n",
    "    print(f\"Training with {m} clients per round\\n\")\n",
    "    \n",
    "    for r in range(rounds):\n",
    "        selected_clients = np.random.choice(range(K), m, replace=False)\n",
    "        \n",
    "        local_weights = []\n",
    "        client_sample_counts = []\n",
    "        client_losses = []\n",
    "        \n",
    "        global_weights = copy.deepcopy(global_model.state_dict())\n",
    "        \n",
    "        # Local training\n",
    "        for client_idx in selected_clients:\n",
    "            if len(client_dict[client_idx]) == 0:\n",
    "                continue\n",
    "                \n",
    "            client = LocalClient(\n",
    "                client_id=client_idx,\n",
    "                dataset=data_builder.train_dataset,\n",
    "                indices=client_dict[client_idx],\n",
    "                device=DEVICE\n",
    "            )\n",
    "            \n",
    "            w_local, loss_local = client.train(global_weights, local_steps=J, lr=lr)\n",
    "            \n",
    "            local_weights.append(w_local)\n",
    "            client_sample_counts.append(len(client_dict[client_idx]))\n",
    "            client_losses.append(loss_local)\n",
    "        \n",
    "        # Aggregation\n",
    "        new_weights = fed_avg_aggregate(global_model, local_weights, client_sample_counts)\n",
    "        global_model.load_state_dict(new_weights, strict=False)\n",
    "        \n",
    "        # Evaluation\n",
    "        test_loss, test_acc = evaluate_global(global_model, test_loader, DEVICE)\n",
    "        avg_train_loss = sum(client_losses) / len(client_losses)\n",
    "        \n",
    "        history['loss'].append(test_loss)\n",
    "        history['accuracy'].append(test_acc)\n",
    "        history['round'].append(r + 1)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if r < 5 or (r + 1) % 5 == 0 or r == rounds - 1:\n",
    "            print(f\"Round {r+1:3d}/{rounds} -> \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "                  f\"Test Loss: {test_loss:.4f} | \"\n",
    "                  f\"Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "    # Final results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training Complete: {dist_type}, J={J}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Final Test Accuracy: {history['accuracy'][-1]:.2f}%\")\n",
    "    print(f\"Best Test Accuracy: {max(history['accuracy']):.2f}%\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return history, global_model, data_builder\n",
    "\n",
    "\n",
    "def run_comprehensive_experiments():\n",
    "    \"\"\"\n",
    "    Run Non-IID experiments only.\n",
    "    \n",
    "    Experiments:\n",
    "    Non-IID with Nc ∈ {1, 5, 10, 50} and J=4, rounds=100\n",
    "    \"\"\"\n",
    "    \n",
    "    K = 100\n",
    "    C = 0.1\n",
    "    J = 4          # Fixed at 4\n",
    "    rounds = 100   # Fixed at 100\n",
    "    lr = 0.1\n",
    "    \n",
    "    # Store all results\n",
    "    all_results = {}\n",
    "    \n",
    "    # Create data builder once\n",
    "    print(\"Creating data builder (will be reused for all experiments)...\")\n",
    "    data_builder = FederatedDataBuilder(val_split_ratio=0.1, K=K)\n",
    "    \n",
    "    # ================================================================\n",
    "    # Non-IID experiments with different Nc values\n",
    "    # ================================================================\n",
    "    \n",
    "    Nc_values = [1, 5, 10, 50]\n",
    "    \n",
    "    for Nc in Nc_values:\n",
    "        exp_name = f\"NonIID_Nc{Nc}_J{J}_R{rounds}\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"EXPERIMENT: Non-IID Nc={Nc}, J={J}, Rounds={rounds}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        history, _, _ = run_fedavg_experiment(\n",
    "            K=K, C=C, J=J, rounds=rounds, lr=lr,\n",
    "            Nc=Nc,\n",
    "            data_builder=data_builder\n",
    "        )\n",
    "        \n",
    "        all_results[exp_name] = history\n",
    "    \n",
    "    # ================================================================\n",
    "    # Plotting Results\n",
    "    # ================================================================\n",
    "    plot_non_iid_results(all_results, Nc_values, J, rounds)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def plot_non_iid_results(all_results, Nc_values, J, rounds):\n",
    "    \"\"\"Plot comparison of Non-IID experiments with different Nc\"\"\"\n",
    "    \n",
    "    # Plot: Effect of Nc (single plot for J=4)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    colors = ['#e74c3c', '#f39c12', '#27ae60', '#3498db']\n",
    "    \n",
    "    for idx, Nc in enumerate(Nc_values):\n",
    "        exp_name = f\"NonIID_Nc{Nc}_J{J}_R{rounds}\"\n",
    "        if exp_name in all_results:\n",
    "            history = all_results[exp_name]\n",
    "            ax.plot(history['round'], history['accuracy'],\n",
    "                   '-o', linewidth=2, markersize=3, \n",
    "                   color=colors[idx], label=f'Nc={Nc}')\n",
    "    \n",
    "    ax.set_title(f'Non-IID FedAvg: Effect of Data Heterogeneity (J={J}, R={rounds})', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Communication Round', fontsize=12)\n",
    "    ax.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('non_iid_nc_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"\\n✓ Saved: non_iid_nc_comparison.png\")\n",
    "    \n",
    "    # Summary table\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL RESULTS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Nc':<8} {'Final Acc':<12} {'Best Acc':<12} {'Best Round':<12}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for Nc in Nc_values:\n",
    "        exp_name = f\"NonIID_Nc{Nc}_J{J}_R{rounds}\"\n",
    "        if exp_name in all_results:\n",
    "            history = all_results[exp_name]\n",
    "            final_acc = history['accuracy'][-1]\n",
    "            best_acc = max(history['accuracy'])\n",
    "            best_round = history['round'][history['accuracy'].index(best_acc)]\n",
    "            print(f\"{Nc:<8} {final_acc:>10.2f}% {best_acc:>10.2f}% {best_round:>11}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Analysis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if len(all_results) == 4:\n",
    "        nc1_acc = all_results[f\"NonIID_Nc1_J{J}_R{rounds}\"]['accuracy'][-1]\n",
    "        nc50_acc = all_results[f\"NonIID_Nc50_J{J}_R{rounds}\"]['accuracy'][-1]\n",
    "        \n",
    "        print(f\"Accuracy degradation from Nc=50 to Nc=1:\")\n",
    "        print(f\"  Nc=50: {nc50_acc:.2f}%\")\n",
    "        print(f\"  Nc=1:  {nc1_acc:.2f}%\")\n",
    "        print(f\"  Drop:  {nc50_acc - nc1_acc:.2f} percentage points\")\n",
    "        print(f\"  Relative: {(1 - nc1_acc/nc50_acc)*100:.1f}% worse\")\n",
    "        \n",
    "        print(f\"\\nData heterogeneity impact:\")\n",
    "        print(f\"  Lower Nc → Higher heterogeneity → Lower accuracy\")\n",
    "        print(f\"  This demonstrates the challenge of Non-IID data in FL\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\"\"\n",
    "    ╔════════════════════════════════════════════════════════════════╗\n",
    "    ║     FedAvg with Non-IID Data Distribution                      ║\n",
    "    ║                                                                ║\n",
    "    ║  Configuration:                                                ║\n",
    "    ║  - K=100 clients, C=0.1 sampling (10 clients/round)          ║\n",
    "    ║  - J=4 local steps (fixed)                                    ║\n",
    "    ║  - Rounds=100 (fixed)                                         ║\n",
    "    ║  - Nc ∈ {1, 5, 10, 50} classes per client                     ║\n",
    "    ║                                                                ║\n",
    "    ║  Total experiments: 4 (one for each Nc value)                 ║\n",
    "    ║  Estimated time: 2-3 hours on T4 GPU                          ║\n",
    "    ╚════════════════════════════════════════════════════════════════╝\n",
    "    \"\"\")\n",
    "    \n",
    "    # Run all Non-IID experiments\n",
    "    all_results = run_comprehensive_experiments()\n",
    "    \n",
    "    print(\"\\n✓ All experiments completed!\")\n",
    "    print(\"\\nGenerated plot:\")\n",
    "    print(\"  - non_iid_nc_comparison.png - Shows effect of data heterogeneity\")\n",
    "    print(\"\\nKey findings:\")\n",
    "    print(\"  - Nc=1 (extreme Non-IID): Lowest accuracy\")\n",
    "    print(\"  - Nc=50 (mild Non-IID): Highest accuracy\")\n",
    "    print(\"  - Clear trend: More heterogeneous data → Lower performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdvancedMachingLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
