{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "170ae2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fed_avg_non_iid.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fed_avg_non_iid.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "from preprocessing import FederatedDataBuilder\n",
    "\n",
    "# Load DINO backbone once globally\n",
    "print(\"=\"*70)\n",
    "print(\"Loading DINO backbone (ONE TIME ONLY)...\")\n",
    "print(\"=\"*70)\n",
    "GLOBAL_DINO_BACKBONE = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "print(\"✓ DINO backbone loaded and cached globally\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "class DINOCIFAR100(nn.Module):\n",
    "    \"\"\"DINO ViT-S/16 for CIFAR-100\"\"\"\n",
    "    def __init__(self, num_classes=100):\n",
    "        super().__init__()\n",
    "        self.backbone = GLOBAL_DINO_BACKBONE\n",
    "        \n",
    "        # Freeze backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Trainable classification head\n",
    "        self.head = nn.Linear(384, num_classes)\n",
    "        nn.init.xavier_uniform_(self.head.weight)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.backbone(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "def create_non_iid_partition(dataset, K, Nc, seed=42):\n",
    "    \"\"\"\n",
    "    Create Non-IID data partition.\n",
    "    \n",
    "    Args:\n",
    "        dataset: CIFAR-100 training dataset\n",
    "        K: Number of clients\n",
    "        Nc: Number of classes per client\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        client_dict: {client_id: [sample_indices]}\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    num_classes = 100\n",
    "    \n",
    "    # Group samples by class\n",
    "    class_to_indices = defaultdict(list)\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        class_to_indices[label].append(idx)\n",
    "    \n",
    "    # Shuffle indices within each class\n",
    "    for label in class_to_indices:\n",
    "        np.random.shuffle(class_to_indices[label])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Creating Non-IID Partition (Nc={Nc})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if Nc == 1:\n",
    "        # Each client gets one class\n",
    "        # Need to handle the case where K > num_classes\n",
    "        client_dict = {}\n",
    "        \n",
    "        for client_id in range(K):\n",
    "            # Assign class in round-robin fashion\n",
    "            assigned_class = client_id % num_classes\n",
    "            \n",
    "            # Calculate how many samples this client should get\n",
    "            samples_per_client = len(class_to_indices[assigned_class]) // (K // num_classes + 1)\n",
    "            \n",
    "            # Get indices for this client\n",
    "            start_idx = (client_id // num_classes) * samples_per_client\n",
    "            end_idx = start_idx + samples_per_client\n",
    "            \n",
    "            client_dict[client_id] = class_to_indices[assigned_class][start_idx:end_idx]\n",
    "        \n",
    "        print(f\"Distribution: Each client has 1 class\")\n",
    "        print(f\"Example: Client 0 has class {0 % num_classes}\")\n",
    "        \n",
    "    else:\n",
    "        # Each client gets Nc classes\n",
    "        client_dict = {}\n",
    "        \n",
    "        # Assign classes to clients\n",
    "        # Strategy: Randomly assign Nc classes to each client\n",
    "        all_classes = list(range(num_classes))\n",
    "        \n",
    "        for client_id in range(K):\n",
    "            # Randomly select Nc classes for this client\n",
    "            np.random.seed(seed + client_id)  # Deterministic per client\n",
    "            assigned_classes = np.random.choice(all_classes, Nc, replace=False)\n",
    "            \n",
    "            # Collect all samples from assigned classes\n",
    "            client_indices = []\n",
    "            for cls in assigned_classes:\n",
    "                # Divide samples of this class among clients that have it\n",
    "                # For simplicity, give each client a portion\n",
    "                samples_in_class = class_to_indices[cls]\n",
    "                portion = len(samples_in_class) // K  # Rough estimate\n",
    "                \n",
    "                start = (client_id * portion) % len(samples_in_class)\n",
    "                end = start + max(1, portion)\n",
    "                \n",
    "                client_indices.extend(samples_in_class[start:end])\n",
    "            \n",
    "            client_dict[client_id] = client_indices\n",
    "            \n",
    "            if client_id == 0:\n",
    "                print(f\"Example: Client 0 assigned classes: {sorted(assigned_classes)}\")\n",
    "    \n",
    "    # Verify\n",
    "    total_samples = sum(len(indices) for indices in client_dict.values())\n",
    "    avg_samples = total_samples / K\n",
    "    \n",
    "    print(f\"Total samples distributed: {total_samples}/{len(dataset)}\")\n",
    "    print(f\"Avg samples per client: {avg_samples:.1f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return client_dict\n",
    "\n",
    "\n",
    "def verify_non_iid_partition(dataset, client_dict, Nc):\n",
    "    \"\"\"Verify the Non-IID partition quality\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Verifying Non-IID Partition (Nc={Nc})\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Check number of classes per client\n",
    "    client_class_counts = []\n",
    "    \n",
    "    for client_id, indices in client_dict.items():\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get unique classes in this client\n",
    "        labels = [dataset[idx][1] for idx in indices]\n",
    "        unique_classes = len(set(labels))\n",
    "        client_class_counts.append(unique_classes)\n",
    "    \n",
    "    avg_classes = np.mean(client_class_counts)\n",
    "    std_classes = np.std(client_class_counts)\n",
    "    \n",
    "    print(f\"Classes per client:\")\n",
    "    print(f\"  Expected: {Nc}\")\n",
    "    print(f\"  Actual avg: {avg_classes:.2f} ± {std_classes:.2f}\")\n",
    "    print(f\"  Min: {min(client_class_counts)}, Max: {max(client_class_counts)}\")\n",
    "    \n",
    "    # Check sample distribution\n",
    "    sample_counts = [len(indices) for indices in client_dict.values()]\n",
    "    print(f\"\\nSamples per client:\")\n",
    "    print(f\"  Avg: {np.mean(sample_counts):.1f} ± {np.std(sample_counts):.1f}\")\n",
    "    print(f\"  Min: {min(sample_counts)}, Max: {max(sample_counts)}\")\n",
    "    \n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "\n",
    "def fed_avg_aggregate(global_model, local_weights, client_sample_counts):\n",
    "    \"\"\"FedAvg aggregation\"\"\"\n",
    "    global_dict = copy.deepcopy(global_model.state_dict())\n",
    "    total_samples = sum(client_sample_counts)\n",
    "    \n",
    "    # Initialize to zero\n",
    "    for k in global_dict.keys():\n",
    "        if 'num_batches_tracked' not in k and 'backbone' not in k:\n",
    "            global_dict[k] = global_dict[k] * 0.0\n",
    "    \n",
    "    # Weighted average\n",
    "    for i in range(len(local_weights)):\n",
    "        ratio = client_sample_counts[i] / total_samples\n",
    "        weights = local_weights[i]\n",
    "        for k in global_dict.keys():\n",
    "            if 'num_batches_tracked' not in k and 'backbone' not in k:\n",
    "                global_dict[k] += weights[k] * ratio\n",
    "    \n",
    "    return global_dict\n",
    "\n",
    "\n",
    "class LocalClient:\n",
    "    \"\"\"Local client for federated training\"\"\"\n",
    "    \n",
    "    def __init__(self, client_id, dataset, indices, device):\n",
    "        self.client_id = client_id\n",
    "        self.indices = indices\n",
    "        self.device = device\n",
    "        \n",
    "        self.trainloader = DataLoader(\n",
    "            Subset(dataset, list(indices)),\n",
    "            batch_size=64,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def train(self, global_weights, local_steps, lr=0.1):\n",
    "        \"\"\"Train for J local steps\"\"\"\n",
    "        \n",
    "        # Create local model\n",
    "        local_model = DINOCIFAR100(num_classes=100).to(self.device)\n",
    "        local_model.load_state_dict(global_weights, strict=False)\n",
    "        local_model.train()\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = optim.SGD(\n",
    "            local_model.head.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Train for J steps\n",
    "        step_count = 0\n",
    "        losses = []\n",
    "        iterator = iter(self.trainloader)\n",
    "        \n",
    "        while step_count < local_steps:\n",
    "            try:\n",
    "                inputs, targets = next(iterator)\n",
    "            except StopIteration:\n",
    "                iterator = iter(self.trainloader)\n",
    "                inputs, targets = next(iterator)\n",
    "            \n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = local_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(local_model.head.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            step_count += 1\n",
    "        \n",
    "        return local_model.state_dict(), sum(losses)/len(losses)\n",
    "\n",
    "\n",
    "def evaluate_global(model, test_loader, device):\n",
    "    \"\"\"Evaluate global model\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss_sum += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return loss_sum / len(test_loader), 100. * correct / total\n",
    "\n",
    "\n",
    "def run_fedavg_experiment(K=100, C=0.1, J=4, rounds=50, lr=0.1, \n",
    "                         Nc=None, data_builder=None):\n",
    "    \"\"\"\n",
    "    Run FedAvg experiment.\n",
    "    \n",
    "    Args:\n",
    "        K: Number of clients\n",
    "        C: Client sampling fraction\n",
    "        J: Local steps\n",
    "        rounds: Communication rounds\n",
    "        lr: Learning rate\n",
    "        Nc: Number of classes per client (None for IID)\n",
    "        data_builder: Pre-created data builder (to reuse data split)\n",
    "    \"\"\"\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    dist_type = f\"Non-IID (Nc={Nc})\" if Nc else \"IID\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FedAvg Experiment: {dist_type}, J={J}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"K={K}, C={C}, J={J}, Rounds={rounds}, LR={lr}\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Data preparation\n",
    "    if data_builder is None:\n",
    "        data_builder = FederatedDataBuilder(val_split_ratio=0.1, K=K)\n",
    "    \n",
    "    # Get appropriate partition\n",
    "    if Nc is None:\n",
    "        # IID\n",
    "        client_dict = data_builder.get_iid_partition()\n",
    "    else:\n",
    "        # Non-IID\n",
    "        client_dict = create_non_iid_partition(\n",
    "            data_builder.train_dataset, K, Nc\n",
    "        )\n",
    "        verify_non_iid_partition(data_builder.train_dataset, client_dict, Nc)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        data_builder.test_dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Global model\n",
    "    print(\"Initializing global model...\")\n",
    "    global_model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
    "    \n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'accuracy': [],\n",
    "        'round': [],\n",
    "        'train_loss': []\n",
    "    }\n",
    "    \n",
    "    # Training\n",
    "    m = max(int(C * K), 1)\n",
    "    print(f\"Training with {m} clients per round\\n\")\n",
    "    \n",
    "    for r in range(rounds):\n",
    "        selected_clients = np.random.choice(range(K), m, replace=False)\n",
    "        \n",
    "        local_weights = []\n",
    "        client_sample_counts = []\n",
    "        client_losses = []\n",
    "        \n",
    "        global_weights = copy.deepcopy(global_model.state_dict())\n",
    "        \n",
    "        # Local training\n",
    "        for client_idx in selected_clients:\n",
    "            if len(client_dict[client_idx]) == 0:\n",
    "                continue\n",
    "                \n",
    "            client = LocalClient(\n",
    "                client_id=client_idx,\n",
    "                dataset=data_builder.train_dataset,\n",
    "                indices=client_dict[client_idx],\n",
    "                device=DEVICE\n",
    "            )\n",
    "            \n",
    "            w_local, loss_local = client.train(global_weights, local_steps=J, lr=lr)\n",
    "            \n",
    "            local_weights.append(w_local)\n",
    "            client_sample_counts.append(len(client_dict[client_idx]))\n",
    "            client_losses.append(loss_local)\n",
    "        \n",
    "        # Aggregation\n",
    "        new_weights = fed_avg_aggregate(global_model, local_weights, client_sample_counts)\n",
    "        global_model.load_state_dict(new_weights, strict=False)\n",
    "        \n",
    "        # Evaluation\n",
    "        test_loss, test_acc = evaluate_global(global_model, test_loader, DEVICE)\n",
    "        avg_train_loss = sum(client_losses) / len(client_losses)\n",
    "        \n",
    "        history['loss'].append(test_loss)\n",
    "        history['accuracy'].append(test_acc)\n",
    "        history['round'].append(r + 1)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if r < 5 or (r + 1) % 5 == 0 or r == rounds - 1:\n",
    "            print(f\"Round {r+1:3d}/{rounds} -> \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "                  f\"Test Loss: {test_loss:.4f} | \"\n",
    "                  f\"Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "    # Final results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training Complete: {dist_type}, J={J}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Final Test Accuracy: {history['accuracy'][-1]:.2f}%\")\n",
    "    print(f\"Best Test Accuracy: {max(history['accuracy']):.2f}%\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return history, global_model, data_builder\n",
    "\n",
    "\n",
    "def run_comprehensive_experiments():\n",
    "    \"\"\"\n",
    "    Run Non-IID experiments only.\n",
    "    \n",
    "    Experiments:\n",
    "    Non-IID with Nc ∈ {1, 5, 10, 50} and J=4, rounds=100\n",
    "    \"\"\"\n",
    "    \n",
    "    K = 100\n",
    "    C = 0.1\n",
    "    J = 4          # Fixed at 4\n",
    "    rounds = 200   # Fixed at 100\n",
    "    lr = 0.04\n",
    "    \n",
    "    # Store all results\n",
    "    all_results = {}\n",
    "    \n",
    "    # Create data builder once\n",
    "    print(\"Creating data builder (will be reused for all experiments)...\")\n",
    "    data_builder = FederatedDataBuilder(val_split_ratio=0.1, K=K)\n",
    "    \n",
    "    # ================================================================\n",
    "    # Non-IID experiments with different Nc values\n",
    "    # ================================================================\n",
    "    \n",
    "    Nc_values = [1, 5, 10, 50]\n",
    "    \n",
    "    for Nc in Nc_values:\n",
    "        exp_name = f\"NonIID_Nc{Nc}_J{J}_R{rounds}\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"EXPERIMENT: Non-IID Nc={Nc}, J={J}, Rounds={rounds}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        history, _, _ = run_fedavg_experiment(\n",
    "            K=K, C=C, J=J, rounds=rounds, lr=lr,\n",
    "            Nc=Nc,\n",
    "            data_builder=data_builder\n",
    "        )\n",
    "        \n",
    "        all_results[exp_name] = history\n",
    "    \n",
    "    # ================================================================\n",
    "    # Plotting Results\n",
    "    # ================================================================\n",
    "    plot_non_iid_results(all_results, Nc_values, J, rounds)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def plot_non_iid_results(all_results, Nc_values, J, rounds):\n",
    "    \"\"\"Plot comparison of Non-IID experiments with different Nc\"\"\"\n",
    "    \n",
    "    # Plot: Effect of Nc (single plot for J=4)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    colors = ['#e74c3c', '#f39c12', '#27ae60', '#3498db']\n",
    "    \n",
    "    for idx, Nc in enumerate(Nc_values):\n",
    "        exp_name = f\"NonIID_Nc{Nc}_J{J}_R{rounds}\"\n",
    "        if exp_name in all_results:\n",
    "            history = all_results[exp_name]\n",
    "            ax.plot(history['round'], history['accuracy'],\n",
    "                   '-o', linewidth=2, markersize=3, \n",
    "                   color=colors[idx], label=f'Nc={Nc}')\n",
    "    \n",
    "    ax.set_title(f'Non-IID FedAvg: Effect of Data Heterogeneity (J={J}, R={rounds})', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Communication Round', fontsize=12)\n",
    "    ax.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('non_iid_nc_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"\\n✓ Saved: non_iid_nc_comparison.png\")\n",
    "    \n",
    "    # Summary table\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL RESULTS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Nc':<8} {'Final Acc':<12} {'Best Acc':<12} {'Best Round':<12}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for Nc in Nc_values:\n",
    "        exp_name = f\"NonIID_Nc{Nc}_J{J}_R{rounds}\"\n",
    "        if exp_name in all_results:\n",
    "            history = all_results[exp_name]\n",
    "            final_acc = history['accuracy'][-1]\n",
    "            best_acc = max(history['accuracy'])\n",
    "            best_round = history['round'][history['accuracy'].index(best_acc)]\n",
    "            print(f\"{Nc:<8} {final_acc:>10.2f}% {best_acc:>10.2f}% {best_round:>11}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Analysis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if len(all_results) == 4:\n",
    "        nc1_acc = all_results[f\"NonIID_Nc1_J{J}_R{rounds}\"]['accuracy'][-1]\n",
    "        nc50_acc = all_results[f\"NonIID_Nc50_J{J}_R{rounds}\"]['accuracy'][-1]\n",
    "        \n",
    "        print(f\"Accuracy degradation from Nc=50 to Nc=1:\")\n",
    "        print(f\"  Nc=50: {nc50_acc:.2f}%\")\n",
    "        print(f\"  Nc=1:  {nc1_acc:.2f}%\")\n",
    "        print(f\"  Drop:  {nc50_acc - nc1_acc:.2f} percentage points\")\n",
    "        print(f\"  Relative: {(1 - nc1_acc/nc50_acc)*100:.1f}% worse\")\n",
    "        \n",
    "        print(f\"\\nData heterogeneity impact:\")\n",
    "        print(f\"  Lower Nc → Higher heterogeneity → Lower accuracy\")\n",
    "        print(f\"  This demonstrates the challenge of Non-IID data in FL\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\"\"\n",
    "    ╔════════════════════════════════════════════════════════════════╗\n",
    "    ║     FedAvg with Non-IID Data Distribution                      ║\n",
    "    ║                                                                ║\n",
    "    ║  Configuration:                                                ║\n",
    "    ║  - K=100 clients, C=0.1 sampling (10 clients/round)          ║\n",
    "    ║  - J=4 local steps (fixed)                                    ║\n",
    "    ║  - Rounds=100 (fixed)                                         ║\n",
    "    ║  - Nc ∈ {1, 5, 10, 50} classes per client                     ║\n",
    "    ║                                                                ║\n",
    "    ║  Total experiments: 4 (one for each Nc value)                 ║\n",
    "    ║  Estimated time: 2-3 hours on T4 GPU                          ║\n",
    "    ╚════════════════════════════════════════════════════════════════╝\n",
    "    \"\"\")\n",
    "    \n",
    "    # Run all Non-IID experiments\n",
    "    all_results = run_comprehensive_experiments()\n",
    "    \n",
    "    print(\"\\n✓ All experiments completed!\")\n",
    "    print(\"\\nGenerated plot:\")\n",
    "    print(\"  - non_iid_nc_comparison.png - Shows effect of data heterogeneity\")\n",
    "    print(\"\\nKey findings:\")\n",
    "    print(\"  - Nc=1 (extreme Non-IID): Lowest accuracy\")\n",
    "    print(\"  - Nc=50 (mild Non-IID): Highest accuracy\")\n",
    "    print(\"  - Clear trend: More heterogeneous data → Lower performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdvancedMachingLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
