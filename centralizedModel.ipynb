{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "825e3347",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "825e3347",
        "outputId": "4919f8b1-bb59-4fc3-974d-99067d13abc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'FederatedDataBuilder' from 'preprocessing' (/content/drive/MyDrive/preprocessing.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3867299641.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/FederatedLearning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFederatedDataBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'FederatedDataBuilder' from 'preprocessing' (/content/drive/MyDrive/preprocessing.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# import module\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive')\n",
        "from preprocessing import FederatedDataBuilder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qo1zSh_3Ur9X",
        "outputId": "2957d479-ac14-4d42-e3a5-fda6632af5ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qo1zSh_3Ur9X",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f172bdc",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "0f172bdc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Assuming FederatedDataBuilder is defined in the previous step\n",
        "# from task_3_1 import FederatedDataBuilder\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Model Definition: DINO ViT-S/16\n",
        "# ---------------------------------------------------------\n",
        "class DINOCIFAR100(nn.Module):\n",
        "    def __init__(self, num_classes=100, freeze_backbone=False):\n",
        "        super(DINOCIFAR100, self).__init__()\n",
        "        # Use the pretrained model architecture DINO ViT-S/16\n",
        "        # Loading from torch hub\n",
        "        print(\"Downloading/Loading DINO ViT-S/16...\")\n",
        "        self.backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
        "\n",
        "        # DINO ViT-S/16 output embedding dimension is 384\n",
        "        self.embed_dim = 384\n",
        "\n",
        "        # Add a classification head for CIFAR-100\n",
        "        self.head = nn.Linear(self.embed_dim, num_classes)\n",
        "\n",
        "        # Option to freeze backbone (Linear Probing) or Fine-tune (Standard)\n",
        "        # The project implies fine-tuning to study parameter sensitivity\n",
        "        if freeze_backbone:\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # DINO forward pass returns the CLS token features\n",
        "        features = self.backbone(x)\n",
        "        output = self.head(features)\n",
        "        return output\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Helper Functions: Train & Evaluate\n",
        "# ---------------------------------------------------------\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Standard training loop for one epoch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # tqdm used for progress bar visualization\n",
        "    for inputs, targets in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / len(loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Standard evaluation loop for validation/test sets.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / len(loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Main Experiment: Centralized Baseline\n",
        "# ---------------------------------------------------------\n",
        "def run_centralized_baseline():\n",
        "    # Centralized baseline\n",
        "    # Search for best hyperparameters\n",
        "    BATCH_SIZE = 64         # Adjust based on Colab GPU memory\n",
        "    EPOCHS = 20             # \"How many epochs do you need?\" -> Monitor convergence\n",
        "    LR = 0.001              # Initial learning rate (tune this using Val set)\n",
        "    MOMENTUM = 0.9          # SGDM Momentum\n",
        "    WEIGHT_DECAY = 1e-4     # Standard regularization\n",
        "\n",
        "    # Check for GPU\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # 1. Prepare Data\n",
        "    # Initialize the builder from Task 3.1\n",
        "    # Split original dataset to have a validation set\n",
        "    data_builder = FederatedDataBuilder(val_split_ratio=0.1)\n",
        "\n",
        "    # Note: ViT usually performs better with 224x224 images.\n",
        "    # If accuracy is very low, consider adding transforms.Resize(224) in the DataBuilder.\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        data_builder.train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        data_builder.val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        data_builder.test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n",
        "    )\n",
        "\n",
        "    # 2. Initialize Model\n",
        "    model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
        "\n",
        "    # 3. Optimizer & Loss\n",
        "    # \"Train your models on CIFAR-100 using the SGDM optimizer\"\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 4. Scheduler\n",
        "    # \"As a learning rate scheduler, we suggest you use the cosine annealing scheduler\"\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    # 5. Training Loop\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    print(f\"Starting centralized training for {EPOCHS} epochs...\")\n",
        "    for epoch in range(EPOCHS):\n",
        "\n",
        "        # Train\n",
        "        t_loss, t_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
        "\n",
        "        # Validate Used for hyperparameter tuning\n",
        "        v_loss, v_acc = evaluate(model, val_loader, criterion, DEVICE)\n",
        "\n",
        "        # Step Scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Logging\n",
        "        history['train_loss'].append(t_loss)\n",
        "        history['train_acc'].append(t_acc)\n",
        "        history['val_loss'].append(v_loss)\n",
        "        history['val_acc'].append(v_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {t_loss:.4f} Acc: {t_acc:.2f}% | Val Loss: {v_loss:.4f} Acc: {v_acc:.2f}%\")\n",
        "\n",
        "    # 6. Final Test Report test loss and test accuracy\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, DEVICE)\n",
        "    print(f\"\\nFinal Test Set Performance -> Loss: {test_loss:.4f} | Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # 7. Plotting Report the plots\n",
        "    plot_results(history)\n",
        "\n",
        "def plot_results(history):\n",
        "    \"\"\"\n",
        "    Helper function to plot loss and accuracy curves.\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
        "    plt.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')\n",
        "    plt.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e41508f",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "4e41508f"
      },
      "outputs": [],
      "source": [
        "# Run the baseline\n",
        "if __name__ == \"__main__\":\n",
        "    run_centralized_baseline()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}