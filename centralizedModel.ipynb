{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ILoveCoder999/FederatedLearning/blob/master/centralizedModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "825e3347",
      "metadata": {
        "id": "825e3347",
        "vscode": {
          "languageId": "plaintext"
        },
        "outputId": "41982631-cd54-46ef-a225-30fdf39b1e7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# import module\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive')\n",
        "from preprocessing import FederatedDataBuilder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0f172bdc",
      "metadata": {
        "id": "0f172bdc",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Assuming FederatedDataBuilder is defined in the previous step\n",
        "# from task_3_1 import FederatedDataBuilder\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Model Definition: DINO ViT-S/16\n",
        "# ---------------------------------------------------------\n",
        "class DINOCIFAR100(nn.Module):\n",
        "    def __init__(self, num_classes=100, freeze_backbone=False):\n",
        "        super(DINOCIFAR100, self).__init__()\n",
        "        # Use the pretrained model architecture DINO ViT-S/16\n",
        "        # Loading from torch hub\n",
        "        print(\"Downloading/Loading DINO ViT-S/16...\")\n",
        "        self.backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
        "\n",
        "        # DINO ViT-S/16 output embedding dimension is 384\n",
        "        self.embed_dim = 384\n",
        "\n",
        "        # Add a classification head for CIFAR-100\n",
        "        self.head = nn.Linear(self.embed_dim, num_classes)\n",
        "\n",
        "        # Option to freeze backbone (Linear Probing) or Fine-tune (Standard)\n",
        "        # The project implies fine-tuning to study parameter sensitivity\n",
        "        if freeze_backbone:\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # DINO forward pass returns the CLS token features\n",
        "        features = self.backbone(x)\n",
        "        output = self.head(features)\n",
        "        return output\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Helper Functions: Train & Evaluate\n",
        "# ---------------------------------------------------------\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Standard training loop for one epoch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # tqdm used for progress bar visualization\n",
        "    for inputs, targets in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / len(loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Standard evaluation loop for validation/test sets.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / len(loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Main Experiment: Centralized Baseline\n",
        "# ---------------------------------------------------------\n",
        "def run_centralized_baseline():\n",
        "    # Centralized baseline\n",
        "    # Search for best hyperparameters\n",
        "    BATCH_SIZE = 32         # Adjust based on Colab GPU memory we try to 64 32 16\n",
        "    EPOCHS = 20             # \"How many epochs do you need?\" -> Monitor convergence\n",
        "    LR = 0.001              # Initial learning rate (tune this using Val set)\n",
        "    MOMENTUM = 0.9          # SGDM Momentum\n",
        "    WEIGHT_DECAY = 1e-4     # Standard regularization\n",
        "\n",
        "    # Check for GPU\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # 1. Prepare Data\n",
        "    # Initialize the builder from Task 3.1\n",
        "    # Split original dataset to have a validation set\n",
        "    data_builder = FederatedDataBuilder(val_split_ratio=0.1)\n",
        "\n",
        "    # Note: ViT usually performs better with 224x224 images.\n",
        "    # If accuracy is very low, consider adding transforms.Resize(224) in the DataBuilder.\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        data_builder.train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        data_builder.val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        data_builder.test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n",
        "    )\n",
        "\n",
        "    # 2. Initialize Model\n",
        "    model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
        "\n",
        "    # 3. Optimizer & Loss\n",
        "    # \"Train your models on CIFAR-100 using the SGDM optimizer\"\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 4. Scheduler\n",
        "    # \"As a learning rate scheduler, we suggest you use the cosine annealing scheduler\"\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    # 5. Training Loop\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    print(f\"Starting centralized training for {EPOCHS} epochs...\")\n",
        "    for epoch in range(EPOCHS):\n",
        "\n",
        "        # Train\n",
        "        t_loss, t_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
        "\n",
        "        # Validate Used for hyperparameter tuning\n",
        "        v_loss, v_acc = evaluate(model, val_loader, criterion, DEVICE)\n",
        "\n",
        "        # Step Scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Logging\n",
        "        history['train_loss'].append(t_loss)\n",
        "        history['train_acc'].append(t_acc)\n",
        "        history['val_loss'].append(v_loss)\n",
        "        history['val_acc'].append(v_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {t_loss:.4f} Acc: {t_acc:.2f}% | Val Loss: {v_loss:.4f} Acc: {v_acc:.2f}%\")\n",
        "\n",
        "    # 6. Final Test Report test loss and test accuracy\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, DEVICE)\n",
        "    print(f\"\\nFinal Test Set Performance -> Loss: {test_loss:.4f} | Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # 7. Plotting Report the plots\n",
        "    plot_results(history)\n",
        "\n",
        "def plot_results(history):\n",
        "    \"\"\"\n",
        "    Helper function to plot loss and accuracy curves.\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
        "    plt.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')\n",
        "    plt.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4e41508f",
      "metadata": {
        "id": "4e41508f",
        "vscode": {
          "languageId": "plaintext"
        },
        "outputId": "97c4adcb-76c1-4e33-89df-218de58246d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:07<00:00, 22.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading/Loading DINO ViT-S/16...\n",
            "Downloading: \"https://github.com/facebookresearch/dino/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dino_deitsmall16_pretrain.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 82.7M/82.7M [00:00<00:00, 112MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting centralized training for 20 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1275801239.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run the baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrun_centralized_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-24553609.py\u001b[0m in \u001b[0;36mrun_centralized_baseline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mt_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Validate Used for hyperparameter tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-24553609.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Run the baseline\n",
        "if __name__ == \"__main__\":\n",
        "    run_centralized_baseline()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}