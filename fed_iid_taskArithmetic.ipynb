{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "47c918b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47c918b0",
        "outputId": "c6618f61-3130-4c75-d911-3f29f1f4aa96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# import module\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive')\n",
        "from preprocessing import FederatedDataBuilder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "69aac5f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "69aac5f0",
        "outputId": "52f0523b-a89f-46b8-82c8-495d93cffca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "######################################################################\n",
            "# Experiment: Sparsity = 0.1\n",
            "######################################################################\n",
            "\n",
            "\n",
            "======================================================================\n",
            "FedAvg + Task Arithmetic\n",
            "======================================================================\n",
            "Clients: 100, Sampling Rate: 0.1\n",
            "Rounds: 20, Local Epochs: 4\n",
            "Sparsity: 0.1, Learning Rate: 0.1\n",
            "Device: cuda\n",
            "======================================================================\n",
            "\n",
            "Preparing data...\n",
            "Creating IID partition for 100 clients...\n",
            "\n",
            "==================================================\n",
            "Verifying Partition\n",
            "==================================================\n",
            "Total samples: 45000/45000\n",
            "No overlap\n",
            "✓ Avg classes per client: 99.0\n",
            "==================================================\n",
            "\n",
            "\n",
            "Initializing global model...\n",
            "Loading DINO backbone (first time only)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ DINO backbone loaded\n",
            "Total params: 21,704,164\n",
            "Trainable params (head): 38,500\n",
            "Frozen params (backbone): 21,665,664\n",
            "\n",
            "Starting training (10 clients per round)...\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Round 1/20\n",
            "======================================================================\n",
            "Selected clients: [40 42 81  5 77]...\n",
            "\n",
            "Client 1/10 (ID: 40):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.9837\n",
            "    Epoch 2/4: loss=15.2387\n",
            "    Epoch 3/4: loss=15.0581\n",
            "    Epoch 4/4: loss=15.1615\n",
            "\n",
            "Client 2/10 (ID: 42):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.3726\n",
            "    Epoch 2/4: loss=15.1657\n",
            "    Epoch 3/4: loss=15.2694\n",
            "    Epoch 4/4: loss=15.1609\n",
            "\n",
            "Client 3/10 (ID: 81):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.4692\n",
            "    Epoch 2/4: loss=15.6224\n",
            "    Epoch 3/4: loss=15.4935\n",
            "    Epoch 4/4: loss=15.6349\n",
            "\n",
            "Client 4/10 (ID: 5):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.0527\n",
            "    Epoch 2/4: loss=15.1437\n",
            "    Epoch 3/4: loss=15.3569\n",
            "    Epoch 4/4: loss=15.2215\n",
            "\n",
            "Client 5/10 (ID: 77):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.1196\n",
            "    Epoch 2/4: loss=15.2585\n",
            "    Epoch 3/4: loss=15.3521\n",
            "    Epoch 4/4: loss=15.0360\n",
            "\n",
            "Client 6/10 (ID: 54):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.9323\n",
            "    Epoch 2/4: loss=14.7375\n",
            "    Epoch 3/4: loss=14.9071\n",
            "    Epoch 4/4: loss=14.9287\n",
            "\n",
            "Client 7/10 (ID: 13):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.7274\n",
            "    Epoch 2/4: loss=15.5513\n",
            "    Epoch 3/4: loss=15.6298\n",
            "    Epoch 4/4: loss=15.6987\n",
            "\n",
            "Client 8/10 (ID: 14):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.9948\n",
            "    Epoch 2/4: loss=14.9872\n",
            "    Epoch 3/4: loss=14.9750\n",
            "    Epoch 4/4: loss=14.8676\n",
            "\n",
            "Client 9/10 (ID: 66):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.7095\n",
            "    Epoch 2/4: loss=14.8409\n",
            "    Epoch 3/4: loss=14.7252\n",
            "    Epoch 4/4: loss=14.5778\n",
            "\n",
            "Client 10/10 (ID: 52):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.4284\n",
            "    Epoch 2/4: loss=14.3704\n",
            "    Epoch 3/4: loss=14.4509\n",
            "    Epoch 4/4: loss=14.3784\n",
            "\n",
            "Aggregating 10 local models...\n",
            "\n",
            "======================================================================\n",
            "Round 1 Global Results:\n",
            "  Test Loss: 15.1756\n",
            "  Test Accuracy: 1.32%\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Round 2/20\n",
            "======================================================================\n",
            "Selected clients: [91 46  1 25 16]...\n",
            "\n",
            "Client 1/10 (ID: 91):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.8011\n",
            "    Epoch 2/4: loss=15.0876\n",
            "    Epoch 3/4: loss=14.9700\n",
            "    Epoch 4/4: loss=14.9768\n",
            "\n",
            "Client 2/10 (ID: 46):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.2356\n",
            "    Epoch 2/4: loss=15.0530\n",
            "    Epoch 3/4: loss=15.3128\n",
            "    Epoch 4/4: loss=15.3180\n",
            "\n",
            "Client 3/10 (ID: 1):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.1731\n",
            "    Epoch 2/4: loss=15.1105\n",
            "    Epoch 3/4: loss=15.0093\n",
            "    Epoch 4/4: loss=14.8894\n",
            "\n",
            "Client 4/10 (ID: 25):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.3668\n",
            "    Epoch 2/4: loss=15.3300\n",
            "    Epoch 3/4: loss=15.2713\n",
            "    Epoch 4/4: loss=15.3516\n",
            "\n",
            "Client 5/10 (ID: 16):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.8861\n",
            "    Epoch 2/4: loss=14.9807\n",
            "    Epoch 3/4: loss=14.8478\n",
            "    Epoch 4/4: loss=14.9015\n",
            "\n",
            "Client 6/10 (ID: 94):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.4415\n",
            "    Epoch 2/4: loss=15.4916\n",
            "    Epoch 3/4: loss=15.5420\n",
            "    Epoch 4/4: loss=15.6211\n",
            "\n",
            "Client 7/10 (ID: 99):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.9279\n",
            "    Epoch 2/4: loss=15.1829\n",
            "    Epoch 3/4: loss=15.0118\n",
            "    Epoch 4/4: loss=15.1735\n",
            "\n",
            "Client 8/10 (ID: 18):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.2526\n",
            "    Epoch 2/4: loss=15.3699\n",
            "    Epoch 3/4: loss=15.3693\n",
            "    Epoch 4/4: loss=15.4223\n",
            "\n",
            "Client 9/10 (ID: 57):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.8281\n",
            "    Epoch 2/4: loss=14.6935\n",
            "    Epoch 3/4: loss=14.7877\n",
            "    Epoch 4/4: loss=14.8942\n",
            "\n",
            "Client 10/10 (ID: 38):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.8764\n",
            "    Epoch 2/4: loss=14.8999\n",
            "    Epoch 3/4: loss=14.6467\n",
            "    Epoch 4/4: loss=14.7689\n",
            "\n",
            "Aggregating 10 local models...\n",
            "\n",
            "======================================================================\n",
            "Round 2 Global Results:\n",
            "  Test Loss: 15.1680\n",
            "  Test Accuracy: 1.32%\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Round 3/20\n",
            "======================================================================\n",
            "Selected clients: [63 17 83 39 57]...\n",
            "\n",
            "Client 1/10 (ID: 63):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.6993\n",
            "    Epoch 2/4: loss=14.7375\n",
            "    Epoch 3/4: loss=14.7007\n",
            "    Epoch 4/4: loss=14.6914\n",
            "\n",
            "Client 2/10 (ID: 17):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.4157\n",
            "    Epoch 2/4: loss=15.3438\n",
            "    Epoch 3/4: loss=15.3084\n",
            "    Epoch 4/4: loss=15.2779\n",
            "\n",
            "Client 3/10 (ID: 83):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.6184\n",
            "    Epoch 2/4: loss=15.6591\n",
            "    Epoch 3/4: loss=15.5634\n",
            "    Epoch 4/4: loss=15.7491\n",
            "\n",
            "Client 4/10 (ID: 39):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.2480\n",
            "    Epoch 2/4: loss=15.3357\n",
            "    Epoch 3/4: loss=15.1415\n",
            "    Epoch 4/4: loss=15.0756\n",
            "\n",
            "Client 5/10 (ID: 57):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.8926\n",
            "    Epoch 2/4: loss=14.8282\n",
            "    Epoch 3/4: loss=14.6289\n",
            "    Epoch 4/4: loss=14.6576\n",
            "\n",
            "Client 6/10 (ID: 33):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.2544\n",
            "    Epoch 2/4: loss=15.2993\n",
            "    Epoch 3/4: loss=15.2989\n",
            "    Epoch 4/4: loss=15.3123\n",
            "\n",
            "Client 7/10 (ID: 75):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.0505\n",
            "    Epoch 2/4: loss=15.0269\n",
            "    Epoch 3/4: loss=14.9768\n",
            "    Epoch 4/4: loss=15.1685\n",
            "\n",
            "Client 8/10 (ID: 37):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.6461\n",
            "    Epoch 2/4: loss=14.8929\n",
            "    Epoch 3/4: loss=14.8386\n",
            "    Epoch 4/4: loss=14.7708\n",
            "\n",
            "Client 9/10 (ID: 87):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.6055\n",
            "    Epoch 2/4: loss=14.5916\n",
            "    Epoch 3/4: loss=14.7299\n",
            "    Epoch 4/4: loss=14.6370\n",
            "\n",
            "Client 10/10 (ID: 68):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.5826\n",
            "    Epoch 2/4: loss=14.6163\n",
            "    Epoch 3/4: loss=14.7719\n",
            "    Epoch 4/4: loss=14.5401\n",
            "\n",
            "Aggregating 10 local models...\n",
            "\n",
            "======================================================================\n",
            "Round 3 Global Results:\n",
            "  Test Loss: 15.1601\n",
            "  Test Accuracy: 1.32%\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Round 4/20\n",
            "======================================================================\n",
            "Selected clients: [56 15 30 79 20]...\n",
            "\n",
            "Client 1/10 (ID: 56):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.2020\n",
            "    Epoch 2/4: loss=15.0912\n",
            "    Epoch 3/4: loss=15.2970\n",
            "    Epoch 4/4: loss=14.9877\n",
            "\n",
            "Client 2/10 (ID: 15):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.1696\n",
            "    Epoch 2/4: loss=15.2456\n",
            "    Epoch 3/4: loss=15.2292\n",
            "    Epoch 4/4: loss=15.1562\n",
            "\n",
            "Client 3/10 (ID: 30):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.5630\n",
            "    Epoch 2/4: loss=14.7779\n",
            "    Epoch 3/4: loss=14.8153\n",
            "    Epoch 4/4: loss=14.7968\n",
            "\n",
            "Client 4/10 (ID: 79):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.4729\n",
            "    Epoch 2/4: loss=15.6160\n",
            "    Epoch 3/4: loss=15.5741\n",
            "    Epoch 4/4: loss=15.5540\n",
            "\n",
            "Client 5/10 (ID: 20):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.0101\n",
            "    Epoch 2/4: loss=14.9973\n",
            "    Epoch 3/4: loss=14.9948\n",
            "    Epoch 4/4: loss=14.8846\n",
            "\n",
            "Client 6/10 (ID: 47):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.8353\n",
            "    Epoch 2/4: loss=14.8964\n",
            "    Epoch 3/4: loss=14.8677\n",
            "    Epoch 4/4: loss=15.0526\n",
            "\n",
            "Client 7/10 (ID: 73):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.1446\n",
            "    Epoch 2/4: loss=15.2561\n",
            "    Epoch 3/4: loss=15.1349\n",
            "    Epoch 4/4: loss=15.2207\n",
            "\n",
            "Client 8/10 (ID: 16):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.9292\n",
            "    Epoch 2/4: loss=14.9682\n",
            "    Epoch 3/4: loss=14.8770\n",
            "    Epoch 4/4: loss=15.0072\n",
            "\n",
            "Client 9/10 (ID: 96):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.7419\n",
            "    Epoch 2/4: loss=15.0664\n",
            "    Epoch 3/4: loss=14.8867\n",
            "    Epoch 4/4: loss=15.0407\n",
            "\n",
            "Client 10/10 (ID: 14):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.9253\n",
            "    Epoch 2/4: loss=14.9376\n",
            "    Epoch 3/4: loss=14.8631\n",
            "    Epoch 4/4: loss=14.8182\n",
            "\n",
            "Aggregating 10 local models...\n",
            "\n",
            "======================================================================\n",
            "Round 4 Global Results:\n",
            "  Test Loss: 15.1526\n",
            "  Test Accuracy: 1.32%\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Round 5/20\n",
            "======================================================================\n",
            "Selected clients: [68 52 81 92 98]...\n",
            "\n",
            "Client 1/10 (ID: 68):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.6209\n",
            "    Epoch 2/4: loss=14.7253\n",
            "    Epoch 3/4: loss=14.6046\n",
            "    Epoch 4/4: loss=14.6831\n",
            "\n",
            "Client 2/10 (ID: 52):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.3728\n",
            "    Epoch 2/4: loss=14.4085\n",
            "    Epoch 3/4: loss=14.4231\n",
            "    Epoch 4/4: loss=14.3234\n",
            "\n",
            "Client 3/10 (ID: 81):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.6713\n",
            "    Epoch 2/4: loss=15.4929\n",
            "    Epoch 3/4: loss=15.6164\n",
            "    Epoch 4/4: loss=15.5377\n",
            "\n",
            "Client 4/10 (ID: 92):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.0329\n",
            "    Epoch 2/4: loss=15.3233\n",
            "    Epoch 3/4: loss=15.3519\n",
            "    Epoch 4/4: loss=15.1269\n",
            "\n",
            "Client 5/10 (ID: 98):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.7282\n",
            "    Epoch 2/4: loss=15.5747\n",
            "    Epoch 3/4: loss=15.5410\n",
            "    Epoch 4/4: loss=15.5905\n",
            "\n",
            "Client 6/10 (ID: 55):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.8763\n",
            "    Epoch 2/4: loss=14.6870\n",
            "    Epoch 3/4: loss=14.9143\n",
            "    Epoch 4/4: loss=14.8678\n",
            "\n",
            "Client 7/10 (ID: 20):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.1457\n",
            "    Epoch 2/4: loss=15.0495\n",
            "    Epoch 3/4: loss=15.1219\n",
            "    Epoch 4/4: loss=15.2231\n",
            "\n",
            "Client 8/10 (ID: 5):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.0251\n",
            "    Epoch 2/4: loss=14.9041\n",
            "    Epoch 3/4: loss=15.1423\n",
            "    Epoch 4/4: loss=15.1482\n",
            "\n",
            "Client 9/10 (ID: 82):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.8114\n",
            "    Epoch 2/4: loss=14.8796\n",
            "    Epoch 3/4: loss=14.9101\n",
            "    Epoch 4/4: loss=14.8759\n",
            "\n",
            "Client 10/10 (ID: 84):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.7640\n",
            "    Epoch 2/4: loss=14.7413\n",
            "    Epoch 3/4: loss=14.6782\n",
            "    Epoch 4/4: loss=14.8023\n",
            "\n",
            "Aggregating 10 local models...\n",
            "\n",
            "======================================================================\n",
            "Round 5 Global Results:\n",
            "  Test Loss: 15.1461\n",
            "  Test Accuracy: 1.32%\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Round 6/20\n",
            "======================================================================\n",
            "Selected clients: [76 10 89  7 42]...\n",
            "\n",
            "Client 1/10 (ID: 76):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.9428\n",
            "    Epoch 2/4: loss=15.0170\n",
            "    Epoch 3/4: loss=15.2250\n",
            "    Epoch 4/4: loss=15.2090\n",
            "\n",
            "Client 2/10 (ID: 10):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.7236\n",
            "    Epoch 2/4: loss=14.9216\n",
            "    Epoch 3/4: loss=14.8040\n",
            "    Epoch 4/4: loss=14.8541\n",
            "\n",
            "Client 3/10 (ID: 89):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.0439\n",
            "    Epoch 2/4: loss=15.1518\n",
            "    Epoch 3/4: loss=15.1044\n",
            "    Epoch 4/4: loss=14.9795\n",
            "\n",
            "Client 4/10 (ID: 7):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.8879\n",
            "    Epoch 2/4: loss=14.7683\n",
            "    Epoch 3/4: loss=14.7625\n",
            "    Epoch 4/4: loss=14.9408\n",
            "\n",
            "Client 5/10 (ID: 42):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.0862\n",
            "    Epoch 2/4: loss=15.1757\n",
            "    Epoch 3/4: loss=15.2708\n",
            "    Epoch 4/4: loss=15.2843\n",
            "\n",
            "Client 6/10 (ID: 4):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.1389\n",
            "    Epoch 2/4: loss=15.2232\n",
            "    Epoch 3/4: loss=15.3042\n",
            "    Epoch 4/4: loss=15.2168\n",
            "\n",
            "Client 7/10 (ID: 68):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.5880\n",
            "    Epoch 2/4: loss=14.6132\n",
            "    Epoch 3/4: loss=14.6239\n",
            "    Epoch 4/4: loss=14.7719\n",
            "\n",
            "Client 8/10 (ID: 84):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.7606\n",
            "    Epoch 2/4: loss=14.8231\n",
            "    Epoch 3/4: loss=14.8434\n",
            "    Epoch 4/4: loss=14.7224\n",
            "\n",
            "Client 9/10 (ID: 58):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.2562\n",
            "    Epoch 2/4: loss=15.2651\n",
            "    Epoch 3/4: loss=15.3176\n",
            "    Epoch 4/4: loss=15.2774\n",
            "\n",
            "Client 10/10 (ID: 85):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.7720\n",
            "    Epoch 2/4: loss=14.7571\n",
            "    Epoch 3/4: loss=14.8656\n",
            "    Epoch 4/4: loss=14.5488\n",
            "\n",
            "Aggregating 10 local models...\n",
            "\n",
            "======================================================================\n",
            "Round 6 Global Results:\n",
            "  Test Loss: 15.1404\n",
            "  Test Accuracy: 1.32%\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Round 7/20\n",
            "======================================================================\n",
            "Selected clients: [35 36 23 37 11]...\n",
            "\n",
            "Client 1/10 (ID: 35):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.4768\n",
            "    Epoch 2/4: loss=15.2832\n",
            "    Epoch 3/4: loss=15.3204\n",
            "    Epoch 4/4: loss=15.2980\n",
            "\n",
            "Client 2/10 (ID: 36):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.4453\n",
            "    Epoch 2/4: loss=15.3745\n",
            "    Epoch 3/4: loss=15.4219\n",
            "    Epoch 4/4: loss=15.4698\n",
            "\n",
            "Client 3/10 (ID: 23):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.2076\n",
            "    Epoch 2/4: loss=15.2606\n",
            "    Epoch 3/4: loss=15.1935\n",
            "    Epoch 4/4: loss=15.1170\n",
            "\n",
            "Client 4/10 (ID: 37):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.8788\n",
            "    Epoch 2/4: loss=14.9517\n",
            "    Epoch 3/4: loss=14.8945\n",
            "    Epoch 4/4: loss=14.8497\n",
            "\n",
            "Client 5/10 (ID: 11):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.4566\n",
            "    Epoch 2/4: loss=15.5387\n",
            "    Epoch 3/4: loss=15.4607\n",
            "    Epoch 4/4: loss=15.6198\n",
            "\n",
            "Client 6/10 (ID: 70):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=14.9647\n",
            "    Epoch 2/4: loss=14.9935\n",
            "    Epoch 3/4: loss=15.1446\n",
            "    Epoch 4/4: loss=14.9244\n",
            "\n",
            "Client 7/10 (ID: 80):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.2900\n",
            "    Epoch 2/4: loss=15.2245\n",
            "    Epoch 3/4: loss=15.1966\n",
            "    Epoch 4/4: loss=15.3113\n",
            "\n",
            "Client 8/10 (ID: 73):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.2656\n",
            "    Epoch 2/4: loss=15.2519\n",
            "    Epoch 3/4: loss=15.2717\n",
            "    Epoch 4/4: loss=15.1414\n",
            "\n",
            "Client 9/10 (ID: 72):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.0178\n",
            "    Epoch 2/4: loss=14.9744\n",
            "    Epoch 3/4: loss=15.0753\n",
            "    Epoch 4/4: loss=15.0727\n",
            "\n",
            "Client 10/10 (ID: 4):\n",
            "  Local data: 450 samples\n",
            "  Computing Fisher sensitivity (head only)...\n",
            "  Computed sensitivity for 2 parameter tensors\n",
            "  Calibrating masks (sparsity=0.1)...\n",
            "  Mask stats: 3850/38500 params active (10.00%)\n",
            "  Sparse fine-tuning (4 epochs)...\n",
            "    Epoch 1/4: loss=15.2553\n",
            "    Epoch 2/4: loss=15.4119\n",
            "    Epoch 3/4: loss=15.2832\n",
            "    Epoch 4/4: loss=15.3332\n",
            "\n",
            "Aggregating 10 local models...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1763361907.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{'#'*70}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         history, model = run_fed_task_arithmetic(\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0mrounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mnum_clients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1763361907.py\u001b[0m in \u001b[0;36mrun_fed_task_arithmetic\u001b[0;34m(rounds, num_clients, sampling_rate, sparsity, local_epochs, lr)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;31m# 全局评估\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1763361907.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loader, device)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return [\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             ]  # Backwards compatibility.\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Task Arithmetic 联邦学习 - 完全修复版本\n",
        "解决了所有已知问题\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from preprocessing import FederatedDataBuilder\n",
        "\n",
        "# ============================================================\n",
        "# 修复后的SparseSGDM\n",
        "# ============================================================\n",
        "class SparseSGDM(optim.SGD):\n",
        "    \"\"\"修复版的稀疏SGD with Momentum\"\"\"\n",
        "    def __init__(self, params, lr=0.001, momentum=0.9, weight_decay=0.0, dampening=0, masks=None):\n",
        "        super().__init__(params, lr=lr, momentum=momentum, weight_decay=weight_decay, dampening=dampening)\n",
        "        self.masks = masks\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                d_p = p.grad\n",
        "\n",
        "                # 应用mask到梯度\n",
        "                if self.masks is not None and p in self.masks:\n",
        "                    d_p = d_p.mul(self.masks[p])\n",
        "\n",
        "                # Weight decay\n",
        "                if group['weight_decay'] != 0:\n",
        "                    d_p = d_p.add(p, alpha=group['weight_decay'])\n",
        "\n",
        "                # Momentum\n",
        "                if group['momentum'] != 0:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'momentum_buffer' not in param_state:\n",
        "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                    else:\n",
        "                        buf = param_state['momentum_buffer']\n",
        "                        buf.mul_(group['momentum']).add_(d_p, alpha=1 - group['dampening'])\n",
        "                    d_p = buf\n",
        "\n",
        "                # 再次应用mask（防止momentum引入被mask的更新）\n",
        "                if self.masks is not None and p in self.masks:\n",
        "                    d_p = d_p.mul(self.masks[p])\n",
        "\n",
        "                # 更新参数\n",
        "                p.add_(d_p, alpha=-group['lr'])\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Fisher敏感度和掩码校准\n",
        "# ============================================================\n",
        "def compute_fisher_sensitivity_head(model, dataloader, criterion, device, num_batches=5):\n",
        "    \"\"\"只对head计算Fisher敏感度\"\"\"\n",
        "    sensitivity = {}\n",
        "    model.eval()\n",
        "\n",
        "    for p in model.head.parameters():\n",
        "        if p.requires_grad:\n",
        "            sensitivity[p] = torch.zeros_like(p.data)\n",
        "\n",
        "    processed = 0\n",
        "    for inputs, labels in dataloader:\n",
        "        if processed >= num_batches:\n",
        "            break\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        model.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        for p in model.head.parameters():\n",
        "            if p.requires_grad and p.grad is not None:\n",
        "                sensitivity[p] += p.grad.data ** 2\n",
        "\n",
        "        processed += 1\n",
        "\n",
        "    for p in sensitivity:\n",
        "        sensitivity[p] /= processed\n",
        "\n",
        "    return sensitivity\n",
        "\n",
        "\n",
        "def calibrate_masks(sensitivity_scores, sparsity_ratio=0.1, keep_least_sensitive=True):\n",
        "    \"\"\"校准掩码\"\"\"\n",
        "    all_scores = torch.cat([s.view(-1) for s in sensitivity_scores.values()])\n",
        "    num_params = all_scores.numel()\n",
        "    k = int(num_params * sparsity_ratio)\n",
        "\n",
        "    if keep_least_sensitive:\n",
        "        threshold = torch.kthvalue(all_scores, k).values.item()\n",
        "        masks = {p: (score <= threshold).float() for p, score in sensitivity_scores.items()}\n",
        "    else:\n",
        "        threshold = torch.kthvalue(all_scores, num_params - k).values.item()\n",
        "        masks = {p: (score >= threshold).float() for p, score in sensitivity_scores.items()}\n",
        "\n",
        "    return masks\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 模型定义\n",
        "# ============================================================\n",
        "GLOBAL_DINO_BACKBONE = None\n",
        "\n",
        "def get_dino_backbone():\n",
        "    global GLOBAL_DINO_BACKBONE\n",
        "    if GLOBAL_DINO_BACKBONE is None:\n",
        "        print(\"Loading DINO backbone...\")\n",
        "        GLOBAL_DINO_BACKBONE = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
        "        print(\"✓ DINO loaded\")\n",
        "    return GLOBAL_DINO_BACKBONE\n",
        "\n",
        "\n",
        "class DINOCIFAR100(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super().__init__()\n",
        "        self.backbone = get_dino_backbone()\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.head = nn.Linear(384, num_classes)\n",
        "        nn.init.xavier_uniform_(self.head.weight)\n",
        "        nn.init.zeros_(self.head.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            features = self.backbone(x)\n",
        "        return self.head(features)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FedAvg聚合\n",
        "# ============================================================\n",
        "def fed_avg_aggregate(global_model, local_weights, client_sample_counts):\n",
        "    global_dict = copy.deepcopy(global_model.state_dict())\n",
        "    total_samples = sum(client_sample_counts)\n",
        "\n",
        "    for k in global_dict.keys():\n",
        "        if 'num_batches_tracked' not in k and 'backbone' not in k:\n",
        "            global_dict[k] = global_dict[k] * 0.0\n",
        "\n",
        "    for i in range(len(local_weights)):\n",
        "        ratio = client_sample_counts[i] / total_samples\n",
        "        weights = local_weights[i]\n",
        "        for k in global_dict.keys():\n",
        "            if 'num_batches_tracked' not in k and 'backbone' not in k:\n",
        "                global_dict[k] += weights[k] * ratio\n",
        "\n",
        "    return global_dict\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 本地训练（Task Arithmetic）\n",
        "# ============================================================\n",
        "def local_train_task_arithmetic(model, train_dataset, client_indices, device,\n",
        "                                 sparsity_ratio=0.1, local_epochs=4, lr=0.1, verbose=False):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    local_sub = Subset(train_dataset, list(client_indices))\n",
        "    local_loader = DataLoader(local_sub, batch_size=128, shuffle=True, num_workers=0)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  Local samples: {len(local_sub)}\")\n",
        "        print(f\"  Computing Fisher sensitivity...\")\n",
        "\n",
        "    # 计算Fisher敏感度（只对head）\n",
        "    sensitivity = compute_fisher_sensitivity_head(model, local_loader, criterion, device, num_batches=5)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  Calibrating masks (sparsity={sparsity_ratio})...\")\n",
        "\n",
        "    # 校准掩码\n",
        "    masks = calibrate_masks(sensitivity, sparsity_ratio=sparsity_ratio, keep_least_sensitive=True)\n",
        "\n",
        "    # 统计\n",
        "    total_params = sum(p.numel() for p in model.head.parameters())\n",
        "    active_params = sum((masks[p] > 0).sum().item() for p in masks)\n",
        "    actual_sparsity = active_params / total_params\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  Active params: {active_params}/{total_params} ({actual_sparsity:.2%})\")\n",
        "        print(f\"  Training for {local_epochs} epochs...\")\n",
        "\n",
        "    # 稀疏微调\n",
        "    model.train()\n",
        "    optimizer = SparseSGDM(model.head.parameters(), lr=lr, momentum=0.9, masks=masks)\n",
        "\n",
        "    losses = []\n",
        "    for epoch in range(local_epochs):\n",
        "        epoch_losses = []\n",
        "        for inputs, labels in local_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.head.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "        epoch_loss = np.mean(epoch_losses)\n",
        "        losses.append(epoch_loss)\n",
        "        if verbose:\n",
        "            print(f\"    Epoch {epoch+1}: loss={epoch_loss:.4f}\")\n",
        "\n",
        "    return model.state_dict(), len(local_sub), np.mean(losses)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 评估函数\n",
        "# ============================================================\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return total_loss / len(loader), 100 * correct / total\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 主联邦训练循环\n",
        "# ============================================================\n",
        "def run_federated_task_arithmetic(rounds=50, num_clients=100, sampling_rate=0.1,\n",
        "                                   sparsity=0.1, local_epochs=4, lr=0.1, verbose_client=False):\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Federated Learning with Task Arithmetic\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"Clients: {num_clients}, Sampling: {sampling_rate}\")\n",
        "    print(f\"Rounds: {rounds}, Local Epochs: {local_epochs}\")\n",
        "    print(f\"Sparsity: {sparsity}, Learning Rate: {lr}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # 数据准备\n",
        "    print(\"Preparing data...\")\n",
        "    builder = FederatedDataBuilder(K=num_clients)\n",
        "    dict_users = builder.get_iid_partition()\n",
        "    test_loader = DataLoader(builder.test_dataset, batch_size=256, shuffle=False, num_workers=0)\n",
        "\n",
        "    # 初始化全局模型\n",
        "    print(\"Initializing global model...\")\n",
        "    global_model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
        "\n",
        "    # 检查初始性能\n",
        "    init_loss, init_acc = evaluate(global_model, test_loader, DEVICE)\n",
        "    print(f\"Initial accuracy: {init_acc:.2f}% (expected ~1% for random init)\\n\")\n",
        "\n",
        "    history = {\"accuracy\": [], \"loss\": [], \"train_loss\": [], \"round\": []}\n",
        "\n",
        "    # 联邦训练\n",
        "    m = max(int(sampling_rate * num_clients), 1)\n",
        "    print(f\"Starting training ({m} clients per round)...\\n\")\n",
        "\n",
        "    for r in range(rounds):\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Round {r+1}/{rounds}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        local_weights = []\n",
        "        local_counts = []\n",
        "        local_losses = []\n",
        "\n",
        "        # 随机选择客户端\n",
        "        selected_clients = np.random.choice(range(num_clients), m, replace=False)\n",
        "        print(f\"Selected clients: {selected_clients[:5]}...\" if m > 5 else f\"Selected: {selected_clients}\")\n",
        "\n",
        "        for idx, client_id in enumerate(selected_clients):\n",
        "            if verbose_client or idx == 0:  # 只详细打印第一个客户端\n",
        "                print(f\"\\nClient {idx+1}/{m} (ID: {client_id}):\")\n",
        "                verbose = True\n",
        "            else:\n",
        "                verbose = False\n",
        "\n",
        "            # 深拷贝全局模型\n",
        "            local_model = copy.deepcopy(global_model)\n",
        "\n",
        "            # 本地训练\n",
        "            w, count, train_loss = local_train_task_arithmetic(\n",
        "                local_model,\n",
        "                builder.train_dataset,\n",
        "                dict_users[client_id],\n",
        "                DEVICE,\n",
        "                sparsity_ratio=sparsity,\n",
        "                local_epochs=local_epochs,\n",
        "                lr=lr,\n",
        "                verbose=verbose\n",
        "            )\n",
        "\n",
        "            local_weights.append(w)\n",
        "            local_counts.append(count)\n",
        "            local_losses.append(train_loss)\n",
        "\n",
        "        # FedAvg聚合\n",
        "        print(f\"\\nAggregating {len(local_weights)} models...\")\n",
        "        global_weights = fed_avg_aggregate(global_model, local_weights, local_counts)\n",
        "        global_model.load_state_dict(global_weights, strict=False)\n",
        "\n",
        "        # 全局评估\n",
        "        test_loss, test_acc = evaluate(global_model, test_loader, DEVICE)\n",
        "        avg_train_loss = np.mean(local_losses)\n",
        "\n",
        "        history[\"accuracy\"].append(test_acc)\n",
        "        history[\"loss\"].append(test_loss)\n",
        "        history[\"train_loss\"].append(avg_train_loss)\n",
        "        history[\"round\"].append(r + 1)\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Round {r+1} Results:\")\n",
        "        print(f\"  Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"  Test Loss: {test_loss:.4f}\")\n",
        "        print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        # 早停检查\n",
        "        if r >= 5 and test_acc < 2:\n",
        "            print(\"⚠️  WARNING: Accuracy still < 2% after 5 rounds!\")\n",
        "            print(\"   This suggests a fundamental problem. Consider debugging.\")\n",
        "\n",
        "    # 最终结果\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training Complete!\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Initial Accuracy: {init_acc:.2f}%\")\n",
        "    print(f\"Final Accuracy: {history['accuracy'][-1]:.2f}%\")\n",
        "    print(f\"Best Accuracy: {max(history['accuracy']):.2f}%\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    return history, global_model\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 主程序\n",
        "# ============================================================\n",
        "if __name__ == \"__main__\":\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # 单个实验\n",
        "    print(\"\\n\" + \"#\"*70)\n",
        "    print(\"# Running Task Arithmetic Experiment\")\n",
        "    print(\"#\"*70 + \"\\n\")\n",
        "\n",
        "    history, model = run_federated_task_arithmetic(\n",
        "        rounds=30,\n",
        "        num_clients=100,\n",
        "        sampling_rate=0.1,\n",
        "        sparsity=0.5,\n",
        "        local_epochs=4,\n",
        "        lr=0.1,\n",
        "        verbose_client=False  # 设为True可以看到所有客户端的详细信息\n",
        "    )\n",
        "\n",
        "    # 绘图\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    ax1.plot(history['round'], history['accuracy'], 'b-o', linewidth=2, markersize=6)\n",
        "    ax1.set_xlabel('Communication Round', fontsize=12)\n",
        "    ax1.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "    ax1.set_title('Test Accuracy vs Round', fontsize=14, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim([0, max(history['accuracy']) * 1.1])\n",
        "\n",
        "    ax2.plot(history['round'], history['train_loss'], 'g-s', label='Train Loss', linewidth=2, markersize=6)\n",
        "    ax2.plot(history['round'], history['loss'], 'r-^', label='Test Loss', linewidth=2, markersize=6)\n",
        "    ax2.set_xlabel('Communication Round', fontsize=12)\n",
        "    ax2.set_ylabel('Loss', fontsize=12)\n",
        "    ax2.set_title('Training Dynamics', fontsize=14, fontweight='bold')\n",
        "    ax2.legend(fontsize=10)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('task_arithmetic_results.png', dpi=150, bbox_inches='tight')\n",
        "    print(\"\\n✓ Figure saved: task_arithmetic_results.png\\n\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "联邦Task Arithmetic诊断脚本 (最终版)\n",
        "用于排查为什么模型精度极低(~1%)\n",
        "\n",
        "使用 DINOCIFAR100 模型类\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "\n",
        "from preprocessing import FederatedDataBuilder\n",
        "from taskarithmetic import compute_fisher_sensitivity, calibrate_masks\n",
        "\n",
        "# 导入模型 - 兼容不同的命名\n",
        "try:\n",
        "    from fed_avg_iid import DINOCIFAR100Fixed as DINOCIFAR100\n",
        "except ImportError:\n",
        "    from fed_avg_iid import DINOCIFAR100\n",
        "\n",
        "\n",
        "def diagnose_mask_problem(sparsity_ratio=0.1):\n",
        "    \"\"\"\n",
        "    诊断掩码是否过于严格\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"诊断 1: 检查掩码生成\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 准备数据\n",
        "    builder = FederatedDataBuilder(K=10)\n",
        "    dict_users = builder.get_iid_partition()\n",
        "\n",
        "    # 创建模型\n",
        "    model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
        "\n",
        "    # 准备一个客户端的数据\n",
        "    local_subset = Subset(builder.train_dataset, list(dict_users[0]))\n",
        "    local_loader = DataLoader(local_subset, batch_size=32, shuffle=True)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 计算敏感度\n",
        "    print(f\"\\n计算Fisher敏感度 (sparsity={sparsity_ratio})...\")\n",
        "    sensitivity_scores = compute_fisher_sensitivity(\n",
        "        model, local_loader, criterion, DEVICE, num_batches=5\n",
        "    )\n",
        "\n",
        "    # 生成掩码\n",
        "    masks = calibrate_masks(\n",
        "        sensitivity_scores,\n",
        "        sparsity_ratio=sparsity_ratio,\n",
        "        keep_least_sensitive=True\n",
        "    )\n",
        "\n",
        "    # 分析掩码\n",
        "    print(\"\\n掩码统计:\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    total_params = 0\n",
        "    frozen_params = 0\n",
        "    active_params = 0\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            mask = masks.get(param)\n",
        "            if mask is not None:\n",
        "                num_params = int(param.numel())\n",
        "                num_active = int(mask.sum().item())\n",
        "                num_frozen = num_params - num_active\n",
        "\n",
        "                total_params += num_params\n",
        "                frozen_params += num_frozen\n",
        "                active_params += num_active\n",
        "\n",
        "                active_ratio = 100 * num_active / num_params\n",
        "                print(f\"{name:30s} | Total: {num_params:8d} | \"\n",
        "                      f\"Active: {num_active:8d} ({active_ratio:5.1f}%) | \"\n",
        "                      f\"Frozen: {num_frozen:8d}\")\n",
        "\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'总计':30s} | Total: {total_params:8d} | \"\n",
        "          f\"Active: {active_params:8d} ({100*active_params/total_params:5.1f}%) | \"\n",
        "          f\"Frozen: {frozen_params:8d}\")\n",
        "\n",
        "    # 关键检查\n",
        "    if active_params == 0:\n",
        "        print(\"\\n❌ 严重错误: 所有参数都被冻结!\")\n",
        "        print(\"   - 模型无法学习\")\n",
        "        print(\"   - 需要检查calibrate_masks实现\")\n",
        "        return False\n",
        "\n",
        "    if active_params < total_params * 0.01:  # 小于1%\n",
        "        print(\"\\n⚠️  警告: 可更新参数过少!\")\n",
        "        print(f\"   - 只有{100*active_params/total_params:.2f}%的参数可以更新\")\n",
        "        print(\"   - 建议增大sparsity_ratio\")\n",
        "        return False\n",
        "\n",
        "    print(\"\\n✓ 掩码生成正常\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def diagnose_training_step():\n",
        "    \"\"\"\n",
        "    诊断单步训练是否正常\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"诊断 2: 检查训练步骤\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 准备数据\n",
        "    builder = FederatedDataBuilder(K=10)\n",
        "    dict_users = builder.get_iid_partition()\n",
        "\n",
        "    # 创建模型\n",
        "    model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
        "\n",
        "    # 检查backbone是否冻结\n",
        "    print(\"\\n检查backbone冻结状态:\")\n",
        "    backbone_params_trainable = sum(p.requires_grad for p in model.backbone.parameters())\n",
        "    print(f\"Backbone可训练参数数: {backbone_params_trainable}\")\n",
        "    if backbone_params_trainable > 0:\n",
        "        print(\"❌ 错误: Backbone应该被完全冻结!\")\n",
        "        return False\n",
        "    print(\"✓ Backbone已正确冻结\")\n",
        "\n",
        "    # 检查head\n",
        "    print(\"\\nHead参数:\")\n",
        "    for name, param in model.head.named_parameters():\n",
        "        print(f\"  {name}: requires_grad={param.requires_grad}, shape={param.shape}\")\n",
        "\n",
        "    # 准备本地数据\n",
        "    local_subset = Subset(builder.train_dataset, list(dict_users[0]))\n",
        "    local_loader = DataLoader(local_subset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # 获取一个batch\n",
        "    inputs, targets = next(iter(local_loader))\n",
        "    inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "    # 前向传播\n",
        "    print(\"\\n测试前向传播:\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        print(f\"  输出形状: {outputs.shape}\")\n",
        "        print(f\"  输出范围: [{outputs.min().item():.2f}, {outputs.max().item():.2f}]\")\n",
        "\n",
        "        # 检查初始精度\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct = predicted.eq(targets).sum().item()\n",
        "        acc = 100. * correct / targets.size(0)\n",
        "        print(f\"  初始精度 (随机): {acc:.2f}%\")\n",
        "\n",
        "        if acc < 0.5 or acc > 5:\n",
        "            print(f\"  ⚠️  警告: 初始精度异常 (期望~1%)\")\n",
        "\n",
        "    # 测试反向传播\n",
        "    print(\"\\n测试反向传播:\")\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 记录初始权重\n",
        "    initial_weight = model.head.weight.clone()\n",
        "\n",
        "    # 训练一步\n",
        "    optimizer = torch.optim.SGD(model.head.parameters(), lr=0.1)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "\n",
        "    # 检查梯度\n",
        "    if model.head.weight.grad is None:\n",
        "        print(\"  ❌ 错误: 没有计算梯度!\")\n",
        "        return False\n",
        "\n",
        "    grad_norm = model.head.weight.grad.norm().item()\n",
        "    print(f\"  梯度范数: {grad_norm:.4f}\")\n",
        "\n",
        "    if grad_norm < 1e-6:\n",
        "        print(\"  ⚠️  警告: 梯度过小\")\n",
        "\n",
        "    # 更新权重\n",
        "    optimizer.step()\n",
        "\n",
        "    # 检查权重是否改变\n",
        "    weight_change = (model.head.weight - initial_weight).abs().max().item()\n",
        "    print(f\"  权重最大变化: {weight_change:.6f}\")\n",
        "\n",
        "    if weight_change < 1e-8:\n",
        "        print(\"  ❌ 错误: 权重没有更新!\")\n",
        "        return False\n",
        "\n",
        "    print(\"  ✓ 训练步骤正常\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def diagnose_aggregation():\n",
        "    \"\"\"\n",
        "    诊断聚合是否正常\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"诊断 3: 检查FedAvg聚合\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    from fed_avg_iid import fed_avg_aggregate\n",
        "\n",
        "    # 创建全局模型\n",
        "    global_model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
        "\n",
        "    # 创建两个模拟的本地模型权重\n",
        "    local_weights = []\n",
        "\n",
        "    for i in range(2):\n",
        "        local_model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
        "        # 随机修改权重\n",
        "        with torch.no_grad():\n",
        "            local_model.head.weight += torch.randn_like(local_model.head.weight) * 0.1\n",
        "        local_weights.append(local_model.state_dict())\n",
        "\n",
        "    client_counts = [100, 100]\n",
        "\n",
        "    # 执行聚合\n",
        "    print(\"\\n执行聚合...\")\n",
        "    global_weight_before = global_model.head.weight.clone()\n",
        "\n",
        "    new_weights = fed_avg_aggregate(global_model, local_weights, client_counts)\n",
        "    global_model.load_state_dict(new_weights, strict=False)\n",
        "\n",
        "    global_weight_after = global_model.head.weight\n",
        "\n",
        "    # 检查权重是否改变\n",
        "    weight_change = (global_weight_after - global_weight_before).abs().max().item()\n",
        "    print(f\"全局模型权重最大变化: {weight_change:.6f}\")\n",
        "\n",
        "    if weight_change < 1e-8:\n",
        "        print(\"❌ 错误: 聚合后全局模型权重没有改变!\")\n",
        "        return False\n",
        "\n",
        "    print(\"✓ 聚合正常\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def test_without_task_arithmetic():\n",
        "    \"\"\"\n",
        "    测试不使用Task Arithmetic的标准FedAvg\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"诊断 4: 测试标准FedAvg (无Task Arithmetic)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 数据准备\n",
        "    builder = FederatedDataBuilder(K=10)\n",
        "    dict_users = builder.get_iid_partition()\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        builder.test_dataset,\n",
        "        batch_size=256,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # 全局模型\n",
        "    global_model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
        "\n",
        "    from fed_avg_iid import fed_avg_aggregate, evaluate_global\n",
        "\n",
        "    print(\"\\n运行3轮标准FedAvg...\")\n",
        "\n",
        "    for r in range(3):\n",
        "        # 选择2个客户端\n",
        "        selected_clients = np.random.choice(range(10), 2, replace=False)\n",
        "\n",
        "        local_weights = []\n",
        "        client_counts = []\n",
        "\n",
        "        for client_idx in selected_clients:\n",
        "            # 本地训练\n",
        "            local_model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
        "            local_model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "            local_subset = Subset(builder.train_dataset, list(dict_users[client_idx]))\n",
        "            local_loader = DataLoader(local_subset, batch_size=32, shuffle=True)\n",
        "\n",
        "            optimizer = torch.optim.SGD(local_model.head.parameters(), lr=0.1, momentum=0.9)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            local_model.train()\n",
        "            step_count = 0\n",
        "            iterator = iter(local_loader)\n",
        "\n",
        "            # 正确实现J=4步\n",
        "            while step_count < 4:\n",
        "                try:\n",
        "                    inputs, targets = next(iterator)\n",
        "                    inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = local_model(inputs)\n",
        "                    loss = criterion(outputs, targets)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    step_count += 1\n",
        "                except StopIteration:\n",
        "                    break\n",
        "\n",
        "            local_weights.append(local_model.state_dict())\n",
        "            client_counts.append(len(dict_users[client_idx]))\n",
        "\n",
        "        # 聚合\n",
        "        new_weights = fed_avg_aggregate(global_model, local_weights, client_counts)\n",
        "        global_model.load_state_dict(new_weights, strict=False)\n",
        "\n",
        "        # 评估\n",
        "        test_loss, test_acc = evaluate_global(global_model, test_loader, DEVICE)\n",
        "        print(f\"Round {r+1}: Test Acc = {test_acc:.2f}%\")\n",
        "\n",
        "        if test_acc < 1.0:\n",
        "            print(\"  ⚠️  精度仍然过低!\")\n",
        "        elif test_acc > 3.0:\n",
        "            print(\"  ✓ 精度开始提升,基础流程正常\")\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    运行所有诊断\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"🔍\"*35)\n",
        "    print(\"      联邦Task Arithmetic 诊断工具\")\n",
        "    print(\"🔍\"*35)\n",
        "\n",
        "    # 诊断1: 掩码\n",
        "    mask_ok = diagnose_mask_problem(sparsity_ratio=0.1)\n",
        "\n",
        "    # 诊断2: 训练步骤\n",
        "    training_ok = diagnose_training_step()\n",
        "\n",
        "    # 诊断3: 聚合\n",
        "    aggregation_ok = diagnose_aggregation()\n",
        "\n",
        "    # 诊断4: 无TA的FedAvg\n",
        "    fedavg_ok = test_without_task_arithmetic()\n",
        "\n",
        "    # 总结\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"诊断总结\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"1. 掩码生成: {'✓ 正常' if mask_ok else '❌ 异常'}\")\n",
        "    print(f\"2. 训练步骤: {'✓ 正常' if training_ok else '❌ 异常'}\")\n",
        "    print(f\"3. FedAvg聚合: {'✓ 正常' if aggregation_ok else '❌ 异常'}\")\n",
        "    print(f\"4. 标准FedAvg: {'✓ 正常' if fedavg_ok else '❌ 异常'}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"建议:\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    if not mask_ok:\n",
        "        print(\"1. 检查calibrate_masks函数实现\")\n",
        "        print(\"2. 尝试更大的sparsity_ratio (如0.5)\")\n",
        "        print(\"3. 确认keep_least_sensitive逻辑正确\")\n",
        "\n",
        "    if not training_ok:\n",
        "        print(\"1. 检查模型初始化\")\n",
        "        print(\"2. 确认backbone正确冻结\")\n",
        "        print(\"3. 调整学习率\")\n",
        "\n",
        "    if not fedavg_ok:\n",
        "        print(\"1. 基础FedAvg就有问题,先修复它\")\n",
        "        print(\"2. 检查数据加载\")\n",
        "        print(\"3. 增加本地训练步数\")\n",
        "\n",
        "    if mask_ok and training_ok and aggregation_ok and not fedavg_ok:\n",
        "        print(\"1. 问题可能在数据处理或模型架构\")\n",
        "        print(\"2. 尝试运行fed_avg_iid.py看是否正常\")\n",
        "\n",
        "    print(\"\\n💡 快速修复建议:\")\n",
        "    print(\"   - 先确保标准FedAvg能work (精度>5%)\")\n",
        "    print(\"   - 再加入Task Arithmetic\")\n",
        "    print(\"   - 使用较大的sparsity_ratio开始测试\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24MgjhUNTAQh",
        "outputId": "8c59d3c7-d232-4177-8ec7-91376c025610"
      },
      "id": "24MgjhUNTAQh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍\n",
            "      联邦Task Arithmetic 诊断工具\n",
            "🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍\n",
            "\n",
            "======================================================================\n",
            "诊断 1: 检查掩码生成\n",
            "======================================================================\n",
            "Creating IID partition for 10 clients...\n",
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "计算Fisher敏感度 (sparsity=0.1)...\n",
            "Calculating sensitivity over 5 batches...\n",
            "\n",
            "掩码统计:\n",
            "----------------------------------------------------------------------\n",
            "backbone.cls_token             | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.pos_embed             | Total:    75648 | Active:      316 (  0.4%) | Frozen:    75332\n",
            "backbone.patch_embed.proj.weight | Total:   294912 | Active:        0 (  0.0%) | Frozen:   294912\n",
            "backbone.patch_embed.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.0.norm1.weight | Total:      384 | Active:      207 ( 53.9%) | Frozen:      177\n",
            "backbone.blocks.0.norm1.bias   | Total:      384 | Active:       44 ( 11.5%) | Frozen:      340\n",
            "backbone.blocks.0.attn.qkv.weight | Total:   442368 | Active:   351693 ( 79.5%) | Frozen:    90675\n",
            "backbone.blocks.0.attn.qkv.bias | Total:     1152 | Active:      685 ( 59.5%) | Frozen:      467\n",
            "backbone.blocks.0.attn.proj.weight | Total:   147456 | Active:    50960 ( 34.6%) | Frozen:    96496\n",
            "backbone.blocks.0.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.0.norm2.weight | Total:      384 | Active:      167 ( 43.5%) | Frozen:      217\n",
            "backbone.blocks.0.norm2.bias   | Total:      384 | Active:       81 ( 21.1%) | Frozen:      303\n",
            "backbone.blocks.0.mlp.fc1.weight | Total:   589824 | Active:   160498 ( 27.2%) | Frozen:   429326\n",
            "backbone.blocks.0.mlp.fc1.bias | Total:     1536 | Active:      416 ( 27.1%) | Frozen:     1120\n",
            "backbone.blocks.0.mlp.fc2.weight | Total:   589824 | Active:     3503 (  0.6%) | Frozen:   586321\n",
            "backbone.blocks.0.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.1.norm1.weight | Total:      384 | Active:      177 ( 46.1%) | Frozen:      207\n",
            "backbone.blocks.1.norm1.bias   | Total:      384 | Active:       19 (  4.9%) | Frozen:      365\n",
            "backbone.blocks.1.attn.qkv.weight | Total:   442368 | Active:   256093 ( 57.9%) | Frozen:   186275\n",
            "backbone.blocks.1.attn.qkv.bias | Total:     1152 | Active:      729 ( 63.3%) | Frozen:      423\n",
            "backbone.blocks.1.attn.proj.weight | Total:   147456 | Active:     6124 (  4.2%) | Frozen:   141332\n",
            "backbone.blocks.1.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.1.norm2.weight | Total:      384 | Active:      228 ( 59.4%) | Frozen:      156\n",
            "backbone.blocks.1.norm2.bias   | Total:      384 | Active:       81 ( 21.1%) | Frozen:      303\n",
            "backbone.blocks.1.mlp.fc1.weight | Total:   589824 | Active:    42255 (  7.2%) | Frozen:   547569\n",
            "backbone.blocks.1.mlp.fc1.bias | Total:     1536 | Active:      105 (  6.8%) | Frozen:     1431\n",
            "backbone.blocks.1.mlp.fc2.weight | Total:   589824 | Active:      446 (  0.1%) | Frozen:   589378\n",
            "backbone.blocks.1.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.2.norm1.weight | Total:      384 | Active:      125 ( 32.6%) | Frozen:      259\n",
            "backbone.blocks.2.norm1.bias   | Total:      384 | Active:        4 (  1.0%) | Frozen:      380\n",
            "backbone.blocks.2.attn.qkv.weight | Total:   442368 | Active:   194159 ( 43.9%) | Frozen:   248209\n",
            "backbone.blocks.2.attn.qkv.bias | Total:     1152 | Active:      679 ( 58.9%) | Frozen:      473\n",
            "backbone.blocks.2.attn.proj.weight | Total:   147456 | Active:      230 (  0.2%) | Frozen:   147226\n",
            "backbone.blocks.2.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.2.norm2.weight | Total:      384 | Active:      161 ( 41.9%) | Frozen:      223\n",
            "backbone.blocks.2.norm2.bias   | Total:      384 | Active:       15 (  3.9%) | Frozen:      369\n",
            "backbone.blocks.2.mlp.fc1.weight | Total:   589824 | Active:    13375 (  2.3%) | Frozen:   576449\n",
            "backbone.blocks.2.mlp.fc1.bias | Total:     1536 | Active:       26 (  1.7%) | Frozen:     1510\n",
            "backbone.blocks.2.mlp.fc2.weight | Total:   589824 | Active:      306 (  0.1%) | Frozen:   589518\n",
            "backbone.blocks.2.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.3.norm1.weight | Total:      384 | Active:      136 ( 35.4%) | Frozen:      248\n",
            "backbone.blocks.3.norm1.bias   | Total:      384 | Active:        4 (  1.0%) | Frozen:      380\n",
            "backbone.blocks.3.attn.qkv.weight | Total:   442368 | Active:   138281 ( 31.3%) | Frozen:   304087\n",
            "backbone.blocks.3.attn.qkv.bias | Total:     1152 | Active:      593 ( 51.5%) | Frozen:      559\n",
            "backbone.blocks.3.attn.proj.weight | Total:   147456 | Active:       76 (  0.1%) | Frozen:   147380\n",
            "backbone.blocks.3.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.3.norm2.weight | Total:      384 | Active:      121 ( 31.5%) | Frozen:      263\n",
            "backbone.blocks.3.norm2.bias   | Total:      384 | Active:        3 (  0.8%) | Frozen:      381\n",
            "backbone.blocks.3.mlp.fc1.weight | Total:   589824 | Active:    11443 (  1.9%) | Frozen:   578381\n",
            "backbone.blocks.3.mlp.fc1.bias | Total:     1536 | Active:       44 (  2.9%) | Frozen:     1492\n",
            "backbone.blocks.3.mlp.fc2.weight | Total:   589824 | Active:       24 (  0.0%) | Frozen:   589800\n",
            "backbone.blocks.3.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.4.norm1.weight | Total:      384 | Active:      100 ( 26.0%) | Frozen:      284\n",
            "backbone.blocks.4.norm1.bias   | Total:      384 | Active:        4 (  1.0%) | Frozen:      380\n",
            "backbone.blocks.4.attn.qkv.weight | Total:   442368 | Active:    94084 ( 21.3%) | Frozen:   348284\n",
            "backbone.blocks.4.attn.qkv.bias | Total:     1152 | Active:      498 ( 43.2%) | Frozen:      654\n",
            "backbone.blocks.4.attn.proj.weight | Total:   147456 | Active:        8 (  0.0%) | Frozen:   147448\n",
            "backbone.blocks.4.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.4.norm2.weight | Total:      384 | Active:      108 ( 28.1%) | Frozen:      276\n",
            "backbone.blocks.4.norm2.bias   | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.4.mlp.fc1.weight | Total:   589824 | Active:    14884 (  2.5%) | Frozen:   574940\n",
            "backbone.blocks.4.mlp.fc1.bias | Total:     1536 | Active:       77 (  5.0%) | Frozen:     1459\n",
            "backbone.blocks.4.mlp.fc2.weight | Total:   589824 | Active:      651 (  0.1%) | Frozen:   589173\n",
            "backbone.blocks.4.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.5.norm1.weight | Total:      384 | Active:      103 ( 26.8%) | Frozen:      281\n",
            "backbone.blocks.5.norm1.bias   | Total:      384 | Active:        2 (  0.5%) | Frozen:      382\n",
            "backbone.blocks.5.attn.qkv.weight | Total:   442368 | Active:    57751 ( 13.1%) | Frozen:   384617\n",
            "backbone.blocks.5.attn.qkv.bias | Total:     1152 | Active:      527 ( 45.7%) | Frozen:      625\n",
            "backbone.blocks.5.attn.proj.weight | Total:   147456 | Active:       10 (  0.0%) | Frozen:   147446\n",
            "backbone.blocks.5.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.5.norm2.weight | Total:      384 | Active:      136 ( 35.4%) | Frozen:      248\n",
            "backbone.blocks.5.norm2.bias   | Total:      384 | Active:        6 (  1.6%) | Frozen:      378\n",
            "backbone.blocks.5.mlp.fc1.weight | Total:   589824 | Active:    15229 (  2.6%) | Frozen:   574595\n",
            "backbone.blocks.5.mlp.fc1.bias | Total:     1536 | Active:      125 (  8.1%) | Frozen:     1411\n",
            "backbone.blocks.5.mlp.fc2.weight | Total:   589824 | Active:      976 (  0.2%) | Frozen:   588848\n",
            "backbone.blocks.5.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.6.norm1.weight | Total:      384 | Active:      107 ( 27.9%) | Frozen:      277\n",
            "backbone.blocks.6.norm1.bias   | Total:      384 | Active:        6 (  1.6%) | Frozen:      378\n",
            "backbone.blocks.6.attn.qkv.weight | Total:   442368 | Active:    52698 ( 11.9%) | Frozen:   389670\n",
            "backbone.blocks.6.attn.qkv.bias | Total:     1152 | Active:      466 ( 40.5%) | Frozen:      686\n",
            "backbone.blocks.6.attn.proj.weight | Total:   147456 | Active:        1 (  0.0%) | Frozen:   147455\n",
            "backbone.blocks.6.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.6.norm2.weight | Total:      384 | Active:      162 ( 42.2%) | Frozen:      222\n",
            "backbone.blocks.6.norm2.bias   | Total:      384 | Active:       18 (  4.7%) | Frozen:      366\n",
            "backbone.blocks.6.mlp.fc1.weight | Total:   589824 | Active:     8376 (  1.4%) | Frozen:   581448\n",
            "backbone.blocks.6.mlp.fc1.bias | Total:     1536 | Active:      153 ( 10.0%) | Frozen:     1383\n",
            "backbone.blocks.6.mlp.fc2.weight | Total:   589824 | Active:     1502 (  0.3%) | Frozen:   588322\n",
            "backbone.blocks.6.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.7.norm1.weight | Total:      384 | Active:      114 ( 29.7%) | Frozen:      270\n",
            "backbone.blocks.7.norm1.bias   | Total:      384 | Active:       13 (  3.4%) | Frozen:      371\n",
            "backbone.blocks.7.attn.qkv.weight | Total:   442368 | Active:    41040 (  9.3%) | Frozen:   401328\n",
            "backbone.blocks.7.attn.qkv.bias | Total:     1152 | Active:      521 ( 45.2%) | Frozen:      631\n",
            "backbone.blocks.7.attn.proj.weight | Total:   147456 | Active:        4 (  0.0%) | Frozen:   147452\n",
            "backbone.blocks.7.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.7.norm2.weight | Total:      384 | Active:      226 ( 58.9%) | Frozen:      158\n",
            "backbone.blocks.7.norm2.bias   | Total:      384 | Active:       70 ( 18.2%) | Frozen:      314\n",
            "backbone.blocks.7.mlp.fc1.weight | Total:   589824 | Active:     7998 (  1.4%) | Frozen:   581826\n",
            "backbone.blocks.7.mlp.fc1.bias | Total:     1536 | Active:      205 ( 13.3%) | Frozen:     1331\n",
            "backbone.blocks.7.mlp.fc2.weight | Total:   589824 | Active:     1180 (  0.2%) | Frozen:   588644\n",
            "backbone.blocks.7.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.8.norm1.weight | Total:      384 | Active:      151 ( 39.3%) | Frozen:      233\n",
            "backbone.blocks.8.norm1.bias   | Total:      384 | Active:       23 (  6.0%) | Frozen:      361\n",
            "backbone.blocks.8.attn.qkv.weight | Total:   442368 | Active:    23144 (  5.2%) | Frozen:   419224\n",
            "backbone.blocks.8.attn.qkv.bias | Total:     1152 | Active:      497 ( 43.1%) | Frozen:      655\n",
            "backbone.blocks.8.attn.proj.weight | Total:   147456 | Active:        0 (  0.0%) | Frozen:   147456\n",
            "backbone.blocks.8.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.8.norm2.weight | Total:      384 | Active:      315 ( 82.0%) | Frozen:       69\n",
            "backbone.blocks.8.norm2.bias   | Total:      384 | Active:      133 ( 34.6%) | Frozen:      251\n",
            "backbone.blocks.8.mlp.fc1.weight | Total:   589824 | Active:    10261 (  1.7%) | Frozen:   579563\n",
            "backbone.blocks.8.mlp.fc1.bias | Total:     1536 | Active:      280 ( 18.2%) | Frozen:     1256\n",
            "backbone.blocks.8.mlp.fc2.weight | Total:   589824 | Active:     2751 (  0.5%) | Frozen:   587073\n",
            "backbone.blocks.8.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.9.norm1.weight | Total:      384 | Active:      163 ( 42.4%) | Frozen:      221\n",
            "backbone.blocks.9.norm1.bias   | Total:      384 | Active:       22 (  5.7%) | Frozen:      362\n",
            "backbone.blocks.9.attn.qkv.weight | Total:   442368 | Active:    13536 (  3.1%) | Frozen:   428832\n",
            "backbone.blocks.9.attn.qkv.bias | Total:     1152 | Active:      556 ( 48.3%) | Frozen:      596\n",
            "backbone.blocks.9.attn.proj.weight | Total:   147456 | Active:        0 (  0.0%) | Frozen:   147456\n",
            "backbone.blocks.9.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.9.norm2.weight | Total:      384 | Active:      381 ( 99.2%) | Frozen:        3\n",
            "backbone.blocks.9.norm2.bias   | Total:      384 | Active:      384 (100.0%) | Frozen:        0\n",
            "backbone.blocks.9.mlp.fc1.weight | Total:   589824 | Active:    81918 ( 13.9%) | Frozen:   507906\n",
            "backbone.blocks.9.mlp.fc1.bias | Total:     1536 | Active:     1327 ( 86.4%) | Frozen:      209\n",
            "backbone.blocks.9.mlp.fc2.weight | Total:   589824 | Active:   198911 ( 33.7%) | Frozen:   390913\n",
            "backbone.blocks.9.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.10.norm1.weight | Total:      384 | Active:      138 ( 35.9%) | Frozen:      246\n",
            "backbone.blocks.10.norm1.bias  | Total:      384 | Active:        2 (  0.5%) | Frozen:      382\n",
            "backbone.blocks.10.attn.qkv.weight | Total:   442368 | Active:    11979 (  2.7%) | Frozen:   430389\n",
            "backbone.blocks.10.attn.qkv.bias | Total:     1152 | Active:      559 ( 48.5%) | Frozen:      593\n",
            "backbone.blocks.10.attn.proj.weight | Total:   147456 | Active:        3 (  0.0%) | Frozen:   147453\n",
            "backbone.blocks.10.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.10.norm2.weight | Total:      384 | Active:      315 ( 82.0%) | Frozen:       69\n",
            "backbone.blocks.10.norm2.bias  | Total:      384 | Active:      223 ( 58.1%) | Frozen:      161\n",
            "backbone.blocks.10.mlp.fc1.weight | Total:   589824 | Active:    31819 (  5.4%) | Frozen:   558005\n",
            "backbone.blocks.10.mlp.fc1.bias | Total:     1536 | Active:      539 ( 35.1%) | Frozen:      997\n",
            "backbone.blocks.10.mlp.fc2.weight | Total:   589824 | Active:    27230 (  4.6%) | Frozen:   562594\n",
            "backbone.blocks.10.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.11.norm1.weight | Total:      384 | Active:      109 ( 28.4%) | Frozen:      275\n",
            "backbone.blocks.11.norm1.bias  | Total:      384 | Active:        4 (  1.0%) | Frozen:      380\n",
            "backbone.blocks.11.attn.qkv.weight | Total:   442368 | Active:    13995 (  3.2%) | Frozen:   428373\n",
            "backbone.blocks.11.attn.qkv.bias | Total:     1152 | Active:      424 ( 36.8%) | Frozen:      728\n",
            "backbone.blocks.11.attn.proj.weight | Total:   147456 | Active:        4 (  0.0%) | Frozen:   147452\n",
            "backbone.blocks.11.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.11.norm2.weight | Total:      384 | Active:      335 ( 87.2%) | Frozen:       49\n",
            "backbone.blocks.11.norm2.bias  | Total:      384 | Active:      307 ( 79.9%) | Frozen:       77\n",
            "backbone.blocks.11.mlp.fc1.weight | Total:   589824 | Active:    99344 ( 16.8%) | Frozen:   490480\n",
            "backbone.blocks.11.mlp.fc1.bias | Total:     1536 | Active:      797 ( 51.9%) | Frozen:      739\n",
            "backbone.blocks.11.mlp.fc2.weight | Total:   589824 | Active:   109163 ( 18.5%) | Frozen:   480661\n",
            "backbone.blocks.11.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.norm.weight           | Total:      384 | Active:        6 (  1.6%) | Frozen:      378\n",
            "backbone.norm.bias             | Total:      384 | Active:        2 (  0.5%) | Frozen:      382\n",
            "head.weight                    | Total:    38400 | Active:     3582 (  9.3%) | Frozen:    34818\n",
            "head.bias                      | Total:      100 | Active:       13 ( 13.0%) | Frozen:       87\n",
            "----------------------------------------------------------------------\n",
            "总计                             | Total: 21704164 | Active:  2170416 ( 10.0%) | Frozen: 19533748\n",
            "\n",
            "✓ 掩码生成正常\n",
            "\n",
            "======================================================================\n",
            "诊断 2: 检查训练步骤\n",
            "======================================================================\n",
            "Creating IID partition for 10 clients...\n",
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "检查backbone冻结状态:\n",
            "Backbone可训练参数数: 150\n",
            "❌ 错误: Backbone应该被完全冻结!\n",
            "\n",
            "======================================================================\n",
            "诊断 3: 检查FedAvg聚合\n",
            "======================================================================\n",
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "执行聚合...\n",
            "全局模型权重最大变化: 0.325009\n",
            "✓ 聚合正常\n",
            "\n",
            "======================================================================\n",
            "诊断 4: 测试标准FedAvg (无Task Arithmetic)\n",
            "======================================================================\n",
            "Creating IID partition for 10 clients...\n",
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "运行3轮标准FedAvg...\n",
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 1: Test Acc = 6.55%\n",
            "  ✓ 精度开始提升,基础流程正常\n",
            "\n",
            "======================================================================\n",
            "诊断总结\n",
            "======================================================================\n",
            "1. 掩码生成: ✓ 正常\n",
            "2. 训练步骤: ❌ 异常\n",
            "3. FedAvg聚合: ✓ 正常\n",
            "4. 标准FedAvg: ✓ 正常\n",
            "\n",
            "======================================================================\n",
            "建议:\n",
            "======================================================================\n",
            "1. 检查模型初始化\n",
            "2. 确认backbone正确冻结\n",
            "3. 调整学习率\n",
            "\n",
            "💡 快速修复建议:\n",
            "   - 先确保标准FedAvg能work (精度>5%)\n",
            "   - 再加入Task Arithmetic\n",
            "   - 使用较大的sparsity_ratio开始测试\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}