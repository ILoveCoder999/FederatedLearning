{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "47c918b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47c918b0",
        "outputId": "dce94327-0b4a-47c3-a598-88eda839f36d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# import module\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive')\n",
        "from preprocessing import FederatedDataBuilder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "69aac5f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "69aac5f0",
        "outputId": "fd1e45be-afd4-4943-ef31-b44777309f6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating IID partition for 100 clients...\n",
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Round 1/20 (Sparsity: 0.5) ---\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Round 1 Global Test Acc: 1.00%\n",
            "\n",
            "--- Round 2/20 (Sparsity: 0.5) ---\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Round 2 Global Test Acc: 1.00%\n",
            "\n",
            "--- Round 3/20 (Sparsity: 0.5) ---\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Round 3 Global Test Acc: 1.00%\n",
            "\n",
            "--- Round 4/20 (Sparsity: 0.5) ---\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Round 4 Global Test Acc: 1.00%\n",
            "\n",
            "--- Round 5/20 (Sparsity: 0.5) ---\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Round 5 Global Test Acc: 1.00%\n",
            "\n",
            "--- Round 6/20 (Sparsity: 0.5) ---\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Round 6 Global Test Acc: 1.00%\n",
            "\n",
            "--- Round 7/20 (Sparsity: 0.5) ---\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Round 7 Global Test Acc: 1.00%\n",
            "\n",
            "--- Round 8/20 (Sparsity: 0.5) ---\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Round 8 Global Test Acc: 1.00%\n",
            "\n",
            "--- Round 9/20 (Sparsity: 0.5) ---\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Round 9 Global Test Acc: 1.00%\n",
            "\n",
            "--- Round 10/20 (Sparsity: 0.5) ---\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Round 10 Global Test Acc: 1.00%\n",
            "\n",
            "--- Round 11/20 (Sparsity: 0.5) ---\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Round 11 Global Test Acc: 1.00%\n",
            "\n",
            "--- Round 12/20 (Sparsity: 0.5) ---\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n",
            "Calculating sensitivity over 5 batches...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3243318728.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;31m# 运行实验\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0mrun_fed_iid_task_arithmetic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparsity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3243318728.py\u001b[0m in \u001b[0;36mrun_fed_iid_task_arithmetic\u001b[0;34m(rounds, num_clients, sampling_rate, sparsity)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m# 本地任务算术训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             w, count = local_train_task_arithmetic_aligned(\n\u001b[0m\u001b[1;32m    177\u001b[0m                 \u001b[0mlocal_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3243318728.py\u001b[0m in \u001b[0;36mlocal_train_task_arithmetic_aligned\u001b[0;34m(model, train_dataset, client_indices, device, sparsity_ratio, local_epochs_backbone, head_warmup_epochs, calib_rounds, calib_batches_per_round, batch_size, lr_head, lr_backbone, momentum)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from preprocessing import FederatedDataBuilder\n",
        "from taskarithmetic import SparseSGDM, compute_fisher_sensitivity, calibrate_masks\n",
        "from fed_avg_iid import DINOCIFAR100, fed_avg_aggregate, GLOBAL_DINO_BACKBONE\n",
        "\n",
        "# ============================================================\n",
        "# 1. 本地训练函数 (集成了 Task Arithmetic)\n",
        "# ============================================================\n",
        "def local_train_task_arithmetic(model, train_dataset, client_indices, device,\n",
        "                                 sparsity_ratio=0.1, local_epochs=4):\n",
        "    \"\"\"\n",
        "    客户端本地训练：包含掩码校准和稀疏微调\n",
        "\n",
        "    关键修复:\n",
        "    1. 只对head层计算敏感度和应用掩码\n",
        "    2. backbone保持冻结\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    # 准备本地数据\n",
        "    local_sub = Subset(train_dataset, list(client_indices))\n",
        "    local_loader = DataLoader(local_sub, batch_size=128, shuffle=True)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # --- 阶段 A: 掩码校准 (Mask Calibration) ---\n",
        "    # 关键修复: 只对head层计算Fisher敏感度\n",
        "    print(f\"  Computing Fisher sensitivity for head parameters only...\")\n",
        "\n",
        "    # 创建一个临时模型，只包含head的参数用于敏感度计算\n",
        "    head_sensitivity = {}\n",
        "\n",
        "    # 计算head层的Fisher敏感度\n",
        "    model.eval()\n",
        "    for p in model.head.parameters():\n",
        "        if p.requires_grad:\n",
        "            head_sensitivity[p] = torch.zeros_like(p.data)\n",
        "\n",
        "    num_batches = min(len(local_loader), 10)  # 限制batch数量以加速\n",
        "    processed = 0\n",
        "\n",
        "    for inputs, labels in local_loader:\n",
        "        if processed >= num_batches:\n",
        "            break\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        model.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # 只累积head的梯度平方\n",
        "        for p in model.head.parameters():\n",
        "            if p.requires_grad and p.grad is not None:\n",
        "                head_sensitivity[p] += p.grad.data ** 2\n",
        "\n",
        "        processed += 1\n",
        "\n",
        "    # 归一化\n",
        "    for p in head_sensitivity:\n",
        "        head_sensitivity[p] /= processed\n",
        "\n",
        "    # 创建掩码\n",
        "    print(f\"  Calibrating masks with sparsity ratio: {sparsity_ratio}\")\n",
        "    masks = calibrate_masks(\n",
        "        head_sensitivity,\n",
        "        sparsity_ratio=sparsity_ratio,\n",
        "        keep_least_sensitive=True\n",
        "    )\n",
        "\n",
        "    # 统计实际更新的参数比例\n",
        "    total_params = sum(p.numel() for p in model.head.parameters())\n",
        "    masked_params = sum((masks[p] > 0).sum().item() for p in masks)\n",
        "    actual_sparsity = masked_params / total_params\n",
        "    print(f\"  Actual sparsity: {actual_sparsity:.2%} ({masked_params}/{total_params} params)\")\n",
        "\n",
        "    # --- 阶段 B: 稀疏微调 (Sparse Fine-tuning) ---\n",
        "    model.train()\n",
        "\n",
        "    # 只优化head参数\n",
        "    optimizer = SparseSGDM(\n",
        "        model.head.parameters(),  # 只优化head\n",
        "        lr=0.1,  # 提高学习率\n",
        "        momentum=0.9,\n",
        "        masks=masks\n",
        "    )\n",
        "\n",
        "    epoch_losses = []\n",
        "    for epoch in range(local_epochs):\n",
        "        batch_losses = []\n",
        "        for inputs, labels in local_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # 梯度裁剪\n",
        "            torch.nn.utils.clip_grad_norm_(model.head.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            batch_losses.append(loss.item())\n",
        "\n",
        "        epoch_loss = np.mean(batch_losses)\n",
        "        epoch_losses.append(epoch_loss)\n",
        "        print(f\"    Epoch {epoch+1}/{local_epochs}: Loss = {epoch_loss:.4f}\")\n",
        "\n",
        "    return model.state_dict(), len(local_sub)\n",
        "\n",
        "# ============================================================\n",
        "# 2. 主联邦训练循环\n",
        "# ============================================================\n",
        "def run_fed_iid_task_arithmetic(rounds=50, num_clients=100, sampling_rate=0.1,\n",
        "                                 sparsity=0.1, local_epochs=4, lr=0.1):\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"FedAvg + Task Arithmetic (Fixed)\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Clients: {num_clients}, Sampling: {sampling_rate}, Rounds: {rounds}\")\n",
        "    print(f\"Sparsity: {sparsity}, Local Epochs: {local_epochs}\")\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # 数据准备\n",
        "    print(\"Preparing data...\")\n",
        "    builder = FederatedDataBuilder(K=num_clients)\n",
        "    dict_users = builder.get_iid_partition()\n",
        "    builder.verify_partition(dict_users)\n",
        "\n",
        "    test_loader = DataLoader(builder.test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "    # 初始化全局模型\n",
        "    print(\"\\nInitializing global model...\")\n",
        "    global_model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
        "    print(\"✓ Global model ready (using shared DINO backbone)\\n\")\n",
        "\n",
        "    history = {\"accuracy\": [], \"loss\": [], \"round\": []}\n",
        "\n",
        "    for r in range(rounds):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Round {r+1}/{rounds} (Sparsity: {sparsity})\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        local_weights = []\n",
        "        local_counts = []\n",
        "\n",
        "        # 随机选择客户端\n",
        "        m = max(int(sampling_rate * num_clients), 1)\n",
        "        selected_clients = np.random.choice(range(num_clients), m, replace=False)\n",
        "        print(f\"Selected {m} clients: {selected_clients[:5]}...\" if m > 5 else f\"Selected clients: {selected_clients}\")\n",
        "\n",
        "        for idx, client_id in enumerate(selected_clients):\n",
        "            print(f\"\\nClient {idx+1}/{m} (ID: {client_id}):\")\n",
        "\n",
        "            # 深拷贝全局模型到本地\n",
        "            local_model = copy.deepcopy(global_model)\n",
        "\n",
        "            # 本地任务算术训练\n",
        "            w, count = local_train_task_arithmetic(\n",
        "                local_model,\n",
        "                builder.train_dataset,\n",
        "                dict_users[client_id],\n",
        "                DEVICE,\n",
        "                sparsity_ratio=sparsity,\n",
        "                local_epochs=local_epochs\n",
        "            )\n",
        "\n",
        "            local_weights.append(w)\n",
        "            local_counts.append(count)\n",
        "\n",
        "        # 聚合 (FedAvg)\n",
        "        print(f\"\\nAggregating {len(local_weights)} local models...\")\n",
        "        global_weights = fed_avg_aggregate(global_model, local_weights, local_counts)\n",
        "        global_model.load_state_dict(global_weights, strict=False)\n",
        "\n",
        "        # 全局评估\n",
        "        test_loss, test_acc = evaluate(global_model, test_loader, DEVICE)\n",
        "        history[\"accuracy\"].append(test_acc)\n",
        "        history[\"loss\"].append(test_loss)\n",
        "        history[\"round\"].append(r + 1)\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Round {r+1} Results:\")\n",
        "        print(f\"  Test Loss: {test_loss:.4f}\")\n",
        "        print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "    # 最终结果\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training Complete!\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Final Test Accuracy: {history['accuracy'][-1]:.2f}%\")\n",
        "    print(f\"Best Test Accuracy: {max(history['accuracy']):.2f}%\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    return history\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return total_loss / len(loader), 100 * correct / total\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 测试不同稀疏度\n",
        "    for sparsity in [0.1, 0.3, 0.5]:\n",
        "        print(f\"\\n\\n{'#'*70}\")\n",
        "        print(f\"# Testing with sparsity = {sparsity}\")\n",
        "        print(f\"{'#'*70}\\n\")\n",
        "\n",
        "        history = run_fed_iid_task_arithmetic(\n",
        "            rounds=30,\n",
        "            num_clients=100,\n",
        "            sampling_rate=0.1,\n",
        "            sparsity=sparsity,\n",
        "            local_epochs=4\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "联邦Task Arithmetic诊断脚本 (最终版)\n",
        "用于排查为什么模型精度极低(~1%)\n",
        "\n",
        "使用 DINOCIFAR100 模型类\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "\n",
        "from preprocessing import FederatedDataBuilder\n",
        "from taskarithmetic import compute_fisher_sensitivity, calibrate_masks\n",
        "\n",
        "# 导入模型 - 兼容不同的命名\n",
        "try:\n",
        "    from fed_avg_iid import DINOCIFAR100Fixed as DINOCIFAR100\n",
        "except ImportError:\n",
        "    from fed_avg_iid import DINOCIFAR100\n",
        "\n",
        "\n",
        "def diagnose_mask_problem(sparsity_ratio=0.1):\n",
        "    \"\"\"\n",
        "    诊断掩码是否过于严格\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"诊断 1: 检查掩码生成\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 准备数据\n",
        "    builder = FederatedDataBuilder(K=10)\n",
        "    dict_users = builder.get_iid_partition()\n",
        "\n",
        "    # 创建模型\n",
        "    model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
        "\n",
        "    # 准备一个客户端的数据\n",
        "    local_subset = Subset(builder.train_dataset, list(dict_users[0]))\n",
        "    local_loader = DataLoader(local_subset, batch_size=32, shuffle=True)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 计算敏感度\n",
        "    print(f\"\\n计算Fisher敏感度 (sparsity={sparsity_ratio})...\")\n",
        "    sensitivity_scores = compute_fisher_sensitivity(\n",
        "        model, local_loader, criterion, DEVICE, num_batches=5\n",
        "    )\n",
        "\n",
        "    # 生成掩码\n",
        "    masks = calibrate_masks(\n",
        "        sensitivity_scores,\n",
        "        sparsity_ratio=sparsity_ratio,\n",
        "        keep_least_sensitive=True\n",
        "    )\n",
        "\n",
        "    # 分析掩码\n",
        "    print(\"\\n掩码统计:\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    total_params = 0\n",
        "    frozen_params = 0\n",
        "    active_params = 0\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            mask = masks.get(param)\n",
        "            if mask is not None:\n",
        "                num_params = int(param.numel())\n",
        "                num_active = int(mask.sum().item())\n",
        "                num_frozen = num_params - num_active\n",
        "\n",
        "                total_params += num_params\n",
        "                frozen_params += num_frozen\n",
        "                active_params += num_active\n",
        "\n",
        "                active_ratio = 100 * num_active / num_params\n",
        "                print(f\"{name:30s} | Total: {num_params:8d} | \"\n",
        "                      f\"Active: {num_active:8d} ({active_ratio:5.1f}%) | \"\n",
        "                      f\"Frozen: {num_frozen:8d}\")\n",
        "\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'总计':30s} | Total: {total_params:8d} | \"\n",
        "          f\"Active: {active_params:8d} ({100*active_params/total_params:5.1f}%) | \"\n",
        "          f\"Frozen: {frozen_params:8d}\")\n",
        "\n",
        "    # 关键检查\n",
        "    if active_params == 0:\n",
        "        print(\"\\n❌ 严重错误: 所有参数都被冻结!\")\n",
        "        print(\"   - 模型无法学习\")\n",
        "        print(\"   - 需要检查calibrate_masks实现\")\n",
        "        return False\n",
        "\n",
        "    if active_params < total_params * 0.01:  # 小于1%\n",
        "        print(\"\\n⚠️  警告: 可更新参数过少!\")\n",
        "        print(f\"   - 只有{100*active_params/total_params:.2f}%的参数可以更新\")\n",
        "        print(\"   - 建议增大sparsity_ratio\")\n",
        "        return False\n",
        "\n",
        "    print(\"\\n✓ 掩码生成正常\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def diagnose_training_step():\n",
        "    \"\"\"\n",
        "    诊断单步训练是否正常\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"诊断 2: 检查训练步骤\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 准备数据\n",
        "    builder = FederatedDataBuilder(K=10)\n",
        "    dict_users = builder.get_iid_partition()\n",
        "\n",
        "    # 创建模型\n",
        "    model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
        "\n",
        "    # 检查backbone是否冻结\n",
        "    print(\"\\n检查backbone冻结状态:\")\n",
        "    backbone_params_trainable = sum(p.requires_grad for p in model.backbone.parameters())\n",
        "    print(f\"Backbone可训练参数数: {backbone_params_trainable}\")\n",
        "    if backbone_params_trainable > 0:\n",
        "        print(\"❌ 错误: Backbone应该被完全冻结!\")\n",
        "        return False\n",
        "    print(\"✓ Backbone已正确冻结\")\n",
        "\n",
        "    # 检查head\n",
        "    print(\"\\nHead参数:\")\n",
        "    for name, param in model.head.named_parameters():\n",
        "        print(f\"  {name}: requires_grad={param.requires_grad}, shape={param.shape}\")\n",
        "\n",
        "    # 准备本地数据\n",
        "    local_subset = Subset(builder.train_dataset, list(dict_users[0]))\n",
        "    local_loader = DataLoader(local_subset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # 获取一个batch\n",
        "    inputs, targets = next(iter(local_loader))\n",
        "    inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "    # 前向传播\n",
        "    print(\"\\n测试前向传播:\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        print(f\"  输出形状: {outputs.shape}\")\n",
        "        print(f\"  输出范围: [{outputs.min().item():.2f}, {outputs.max().item():.2f}]\")\n",
        "\n",
        "        # 检查初始精度\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct = predicted.eq(targets).sum().item()\n",
        "        acc = 100. * correct / targets.size(0)\n",
        "        print(f\"  初始精度 (随机): {acc:.2f}%\")\n",
        "\n",
        "        if acc < 0.5 or acc > 5:\n",
        "            print(f\"  ⚠️  警告: 初始精度异常 (期望~1%)\")\n",
        "\n",
        "    # 测试反向传播\n",
        "    print(\"\\n测试反向传播:\")\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 记录初始权重\n",
        "    initial_weight = model.head.weight.clone()\n",
        "\n",
        "    # 训练一步\n",
        "    optimizer = torch.optim.SGD(model.head.parameters(), lr=0.1)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "\n",
        "    # 检查梯度\n",
        "    if model.head.weight.grad is None:\n",
        "        print(\"  ❌ 错误: 没有计算梯度!\")\n",
        "        return False\n",
        "\n",
        "    grad_norm = model.head.weight.grad.norm().item()\n",
        "    print(f\"  梯度范数: {grad_norm:.4f}\")\n",
        "\n",
        "    if grad_norm < 1e-6:\n",
        "        print(\"  ⚠️  警告: 梯度过小\")\n",
        "\n",
        "    # 更新权重\n",
        "    optimizer.step()\n",
        "\n",
        "    # 检查权重是否改变\n",
        "    weight_change = (model.head.weight - initial_weight).abs().max().item()\n",
        "    print(f\"  权重最大变化: {weight_change:.6f}\")\n",
        "\n",
        "    if weight_change < 1e-8:\n",
        "        print(\"  ❌ 错误: 权重没有更新!\")\n",
        "        return False\n",
        "\n",
        "    print(\"  ✓ 训练步骤正常\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def diagnose_aggregation():\n",
        "    \"\"\"\n",
        "    诊断聚合是否正常\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"诊断 3: 检查FedAvg聚合\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    from fed_avg_iid import fed_avg_aggregate\n",
        "\n",
        "    # 创建全局模型\n",
        "    global_model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
        "\n",
        "    # 创建两个模拟的本地模型权重\n",
        "    local_weights = []\n",
        "\n",
        "    for i in range(2):\n",
        "        local_model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
        "        # 随机修改权重\n",
        "        with torch.no_grad():\n",
        "            local_model.head.weight += torch.randn_like(local_model.head.weight) * 0.1\n",
        "        local_weights.append(local_model.state_dict())\n",
        "\n",
        "    client_counts = [100, 100]\n",
        "\n",
        "    # 执行聚合\n",
        "    print(\"\\n执行聚合...\")\n",
        "    global_weight_before = global_model.head.weight.clone()\n",
        "\n",
        "    new_weights = fed_avg_aggregate(global_model, local_weights, client_counts)\n",
        "    global_model.load_state_dict(new_weights, strict=False)\n",
        "\n",
        "    global_weight_after = global_model.head.weight\n",
        "\n",
        "    # 检查权重是否改变\n",
        "    weight_change = (global_weight_after - global_weight_before).abs().max().item()\n",
        "    print(f\"全局模型权重最大变化: {weight_change:.6f}\")\n",
        "\n",
        "    if weight_change < 1e-8:\n",
        "        print(\"❌ 错误: 聚合后全局模型权重没有改变!\")\n",
        "        return False\n",
        "\n",
        "    print(\"✓ 聚合正常\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def test_without_task_arithmetic():\n",
        "    \"\"\"\n",
        "    测试不使用Task Arithmetic的标准FedAvg\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"诊断 4: 测试标准FedAvg (无Task Arithmetic)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 数据准备\n",
        "    builder = FederatedDataBuilder(K=10)\n",
        "    dict_users = builder.get_iid_partition()\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        builder.test_dataset,\n",
        "        batch_size=256,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # 全局模型\n",
        "    global_model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
        "\n",
        "    from fed_avg_iid import fed_avg_aggregate, evaluate_global\n",
        "\n",
        "    print(\"\\n运行3轮标准FedAvg...\")\n",
        "\n",
        "    for r in range(3):\n",
        "        # 选择2个客户端\n",
        "        selected_clients = np.random.choice(range(10), 2, replace=False)\n",
        "\n",
        "        local_weights = []\n",
        "        client_counts = []\n",
        "\n",
        "        for client_idx in selected_clients:\n",
        "            # 本地训练\n",
        "            local_model = DINOCIFAR100(num_classes=100).to(DEVICE)\n",
        "            local_model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "            local_subset = Subset(builder.train_dataset, list(dict_users[client_idx]))\n",
        "            local_loader = DataLoader(local_subset, batch_size=32, shuffle=True)\n",
        "\n",
        "            optimizer = torch.optim.SGD(local_model.head.parameters(), lr=0.1, momentum=0.9)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            local_model.train()\n",
        "            step_count = 0\n",
        "            iterator = iter(local_loader)\n",
        "\n",
        "            # 正确实现J=4步\n",
        "            while step_count < 4:\n",
        "                try:\n",
        "                    inputs, targets = next(iterator)\n",
        "                    inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = local_model(inputs)\n",
        "                    loss = criterion(outputs, targets)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    step_count += 1\n",
        "                except StopIteration:\n",
        "                    break\n",
        "\n",
        "            local_weights.append(local_model.state_dict())\n",
        "            client_counts.append(len(dict_users[client_idx]))\n",
        "\n",
        "        # 聚合\n",
        "        new_weights = fed_avg_aggregate(global_model, local_weights, client_counts)\n",
        "        global_model.load_state_dict(new_weights, strict=False)\n",
        "\n",
        "        # 评估\n",
        "        test_loss, test_acc = evaluate_global(global_model, test_loader, DEVICE)\n",
        "        print(f\"Round {r+1}: Test Acc = {test_acc:.2f}%\")\n",
        "\n",
        "        if test_acc < 1.0:\n",
        "            print(\"  ⚠️  精度仍然过低!\")\n",
        "        elif test_acc > 3.0:\n",
        "            print(\"  ✓ 精度开始提升,基础流程正常\")\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    运行所有诊断\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"🔍\"*35)\n",
        "    print(\"      联邦Task Arithmetic 诊断工具\")\n",
        "    print(\"🔍\"*35)\n",
        "\n",
        "    # 诊断1: 掩码\n",
        "    mask_ok = diagnose_mask_problem(sparsity_ratio=0.1)\n",
        "\n",
        "    # 诊断2: 训练步骤\n",
        "    training_ok = diagnose_training_step()\n",
        "\n",
        "    # 诊断3: 聚合\n",
        "    aggregation_ok = diagnose_aggregation()\n",
        "\n",
        "    # 诊断4: 无TA的FedAvg\n",
        "    fedavg_ok = test_without_task_arithmetic()\n",
        "\n",
        "    # 总结\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"诊断总结\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"1. 掩码生成: {'✓ 正常' if mask_ok else '❌ 异常'}\")\n",
        "    print(f\"2. 训练步骤: {'✓ 正常' if training_ok else '❌ 异常'}\")\n",
        "    print(f\"3. FedAvg聚合: {'✓ 正常' if aggregation_ok else '❌ 异常'}\")\n",
        "    print(f\"4. 标准FedAvg: {'✓ 正常' if fedavg_ok else '❌ 异常'}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"建议:\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    if not mask_ok:\n",
        "        print(\"1. 检查calibrate_masks函数实现\")\n",
        "        print(\"2. 尝试更大的sparsity_ratio (如0.5)\")\n",
        "        print(\"3. 确认keep_least_sensitive逻辑正确\")\n",
        "\n",
        "    if not training_ok:\n",
        "        print(\"1. 检查模型初始化\")\n",
        "        print(\"2. 确认backbone正确冻结\")\n",
        "        print(\"3. 调整学习率\")\n",
        "\n",
        "    if not fedavg_ok:\n",
        "        print(\"1. 基础FedAvg就有问题,先修复它\")\n",
        "        print(\"2. 检查数据加载\")\n",
        "        print(\"3. 增加本地训练步数\")\n",
        "\n",
        "    if mask_ok and training_ok and aggregation_ok and not fedavg_ok:\n",
        "        print(\"1. 问题可能在数据处理或模型架构\")\n",
        "        print(\"2. 尝试运行fed_avg_iid.py看是否正常\")\n",
        "\n",
        "    print(\"\\n💡 快速修复建议:\")\n",
        "    print(\"   - 先确保标准FedAvg能work (精度>5%)\")\n",
        "    print(\"   - 再加入Task Arithmetic\")\n",
        "    print(\"   - 使用较大的sparsity_ratio开始测试\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24MgjhUNTAQh",
        "outputId": "8c59d3c7-d232-4177-8ec7-91376c025610"
      },
      "id": "24MgjhUNTAQh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍\n",
            "      联邦Task Arithmetic 诊断工具\n",
            "🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍🔍\n",
            "\n",
            "======================================================================\n",
            "诊断 1: 检查掩码生成\n",
            "======================================================================\n",
            "Creating IID partition for 10 clients...\n",
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "计算Fisher敏感度 (sparsity=0.1)...\n",
            "Calculating sensitivity over 5 batches...\n",
            "\n",
            "掩码统计:\n",
            "----------------------------------------------------------------------\n",
            "backbone.cls_token             | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.pos_embed             | Total:    75648 | Active:      316 (  0.4%) | Frozen:    75332\n",
            "backbone.patch_embed.proj.weight | Total:   294912 | Active:        0 (  0.0%) | Frozen:   294912\n",
            "backbone.patch_embed.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.0.norm1.weight | Total:      384 | Active:      207 ( 53.9%) | Frozen:      177\n",
            "backbone.blocks.0.norm1.bias   | Total:      384 | Active:       44 ( 11.5%) | Frozen:      340\n",
            "backbone.blocks.0.attn.qkv.weight | Total:   442368 | Active:   351693 ( 79.5%) | Frozen:    90675\n",
            "backbone.blocks.0.attn.qkv.bias | Total:     1152 | Active:      685 ( 59.5%) | Frozen:      467\n",
            "backbone.blocks.0.attn.proj.weight | Total:   147456 | Active:    50960 ( 34.6%) | Frozen:    96496\n",
            "backbone.blocks.0.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.0.norm2.weight | Total:      384 | Active:      167 ( 43.5%) | Frozen:      217\n",
            "backbone.blocks.0.norm2.bias   | Total:      384 | Active:       81 ( 21.1%) | Frozen:      303\n",
            "backbone.blocks.0.mlp.fc1.weight | Total:   589824 | Active:   160498 ( 27.2%) | Frozen:   429326\n",
            "backbone.blocks.0.mlp.fc1.bias | Total:     1536 | Active:      416 ( 27.1%) | Frozen:     1120\n",
            "backbone.blocks.0.mlp.fc2.weight | Total:   589824 | Active:     3503 (  0.6%) | Frozen:   586321\n",
            "backbone.blocks.0.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.1.norm1.weight | Total:      384 | Active:      177 ( 46.1%) | Frozen:      207\n",
            "backbone.blocks.1.norm1.bias   | Total:      384 | Active:       19 (  4.9%) | Frozen:      365\n",
            "backbone.blocks.1.attn.qkv.weight | Total:   442368 | Active:   256093 ( 57.9%) | Frozen:   186275\n",
            "backbone.blocks.1.attn.qkv.bias | Total:     1152 | Active:      729 ( 63.3%) | Frozen:      423\n",
            "backbone.blocks.1.attn.proj.weight | Total:   147456 | Active:     6124 (  4.2%) | Frozen:   141332\n",
            "backbone.blocks.1.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.1.norm2.weight | Total:      384 | Active:      228 ( 59.4%) | Frozen:      156\n",
            "backbone.blocks.1.norm2.bias   | Total:      384 | Active:       81 ( 21.1%) | Frozen:      303\n",
            "backbone.blocks.1.mlp.fc1.weight | Total:   589824 | Active:    42255 (  7.2%) | Frozen:   547569\n",
            "backbone.blocks.1.mlp.fc1.bias | Total:     1536 | Active:      105 (  6.8%) | Frozen:     1431\n",
            "backbone.blocks.1.mlp.fc2.weight | Total:   589824 | Active:      446 (  0.1%) | Frozen:   589378\n",
            "backbone.blocks.1.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.2.norm1.weight | Total:      384 | Active:      125 ( 32.6%) | Frozen:      259\n",
            "backbone.blocks.2.norm1.bias   | Total:      384 | Active:        4 (  1.0%) | Frozen:      380\n",
            "backbone.blocks.2.attn.qkv.weight | Total:   442368 | Active:   194159 ( 43.9%) | Frozen:   248209\n",
            "backbone.blocks.2.attn.qkv.bias | Total:     1152 | Active:      679 ( 58.9%) | Frozen:      473\n",
            "backbone.blocks.2.attn.proj.weight | Total:   147456 | Active:      230 (  0.2%) | Frozen:   147226\n",
            "backbone.blocks.2.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.2.norm2.weight | Total:      384 | Active:      161 ( 41.9%) | Frozen:      223\n",
            "backbone.blocks.2.norm2.bias   | Total:      384 | Active:       15 (  3.9%) | Frozen:      369\n",
            "backbone.blocks.2.mlp.fc1.weight | Total:   589824 | Active:    13375 (  2.3%) | Frozen:   576449\n",
            "backbone.blocks.2.mlp.fc1.bias | Total:     1536 | Active:       26 (  1.7%) | Frozen:     1510\n",
            "backbone.blocks.2.mlp.fc2.weight | Total:   589824 | Active:      306 (  0.1%) | Frozen:   589518\n",
            "backbone.blocks.2.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.3.norm1.weight | Total:      384 | Active:      136 ( 35.4%) | Frozen:      248\n",
            "backbone.blocks.3.norm1.bias   | Total:      384 | Active:        4 (  1.0%) | Frozen:      380\n",
            "backbone.blocks.3.attn.qkv.weight | Total:   442368 | Active:   138281 ( 31.3%) | Frozen:   304087\n",
            "backbone.blocks.3.attn.qkv.bias | Total:     1152 | Active:      593 ( 51.5%) | Frozen:      559\n",
            "backbone.blocks.3.attn.proj.weight | Total:   147456 | Active:       76 (  0.1%) | Frozen:   147380\n",
            "backbone.blocks.3.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.3.norm2.weight | Total:      384 | Active:      121 ( 31.5%) | Frozen:      263\n",
            "backbone.blocks.3.norm2.bias   | Total:      384 | Active:        3 (  0.8%) | Frozen:      381\n",
            "backbone.blocks.3.mlp.fc1.weight | Total:   589824 | Active:    11443 (  1.9%) | Frozen:   578381\n",
            "backbone.blocks.3.mlp.fc1.bias | Total:     1536 | Active:       44 (  2.9%) | Frozen:     1492\n",
            "backbone.blocks.3.mlp.fc2.weight | Total:   589824 | Active:       24 (  0.0%) | Frozen:   589800\n",
            "backbone.blocks.3.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.4.norm1.weight | Total:      384 | Active:      100 ( 26.0%) | Frozen:      284\n",
            "backbone.blocks.4.norm1.bias   | Total:      384 | Active:        4 (  1.0%) | Frozen:      380\n",
            "backbone.blocks.4.attn.qkv.weight | Total:   442368 | Active:    94084 ( 21.3%) | Frozen:   348284\n",
            "backbone.blocks.4.attn.qkv.bias | Total:     1152 | Active:      498 ( 43.2%) | Frozen:      654\n",
            "backbone.blocks.4.attn.proj.weight | Total:   147456 | Active:        8 (  0.0%) | Frozen:   147448\n",
            "backbone.blocks.4.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.4.norm2.weight | Total:      384 | Active:      108 ( 28.1%) | Frozen:      276\n",
            "backbone.blocks.4.norm2.bias   | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.4.mlp.fc1.weight | Total:   589824 | Active:    14884 (  2.5%) | Frozen:   574940\n",
            "backbone.blocks.4.mlp.fc1.bias | Total:     1536 | Active:       77 (  5.0%) | Frozen:     1459\n",
            "backbone.blocks.4.mlp.fc2.weight | Total:   589824 | Active:      651 (  0.1%) | Frozen:   589173\n",
            "backbone.blocks.4.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.5.norm1.weight | Total:      384 | Active:      103 ( 26.8%) | Frozen:      281\n",
            "backbone.blocks.5.norm1.bias   | Total:      384 | Active:        2 (  0.5%) | Frozen:      382\n",
            "backbone.blocks.5.attn.qkv.weight | Total:   442368 | Active:    57751 ( 13.1%) | Frozen:   384617\n",
            "backbone.blocks.5.attn.qkv.bias | Total:     1152 | Active:      527 ( 45.7%) | Frozen:      625\n",
            "backbone.blocks.5.attn.proj.weight | Total:   147456 | Active:       10 (  0.0%) | Frozen:   147446\n",
            "backbone.blocks.5.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.5.norm2.weight | Total:      384 | Active:      136 ( 35.4%) | Frozen:      248\n",
            "backbone.blocks.5.norm2.bias   | Total:      384 | Active:        6 (  1.6%) | Frozen:      378\n",
            "backbone.blocks.5.mlp.fc1.weight | Total:   589824 | Active:    15229 (  2.6%) | Frozen:   574595\n",
            "backbone.blocks.5.mlp.fc1.bias | Total:     1536 | Active:      125 (  8.1%) | Frozen:     1411\n",
            "backbone.blocks.5.mlp.fc2.weight | Total:   589824 | Active:      976 (  0.2%) | Frozen:   588848\n",
            "backbone.blocks.5.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.6.norm1.weight | Total:      384 | Active:      107 ( 27.9%) | Frozen:      277\n",
            "backbone.blocks.6.norm1.bias   | Total:      384 | Active:        6 (  1.6%) | Frozen:      378\n",
            "backbone.blocks.6.attn.qkv.weight | Total:   442368 | Active:    52698 ( 11.9%) | Frozen:   389670\n",
            "backbone.blocks.6.attn.qkv.bias | Total:     1152 | Active:      466 ( 40.5%) | Frozen:      686\n",
            "backbone.blocks.6.attn.proj.weight | Total:   147456 | Active:        1 (  0.0%) | Frozen:   147455\n",
            "backbone.blocks.6.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.6.norm2.weight | Total:      384 | Active:      162 ( 42.2%) | Frozen:      222\n",
            "backbone.blocks.6.norm2.bias   | Total:      384 | Active:       18 (  4.7%) | Frozen:      366\n",
            "backbone.blocks.6.mlp.fc1.weight | Total:   589824 | Active:     8376 (  1.4%) | Frozen:   581448\n",
            "backbone.blocks.6.mlp.fc1.bias | Total:     1536 | Active:      153 ( 10.0%) | Frozen:     1383\n",
            "backbone.blocks.6.mlp.fc2.weight | Total:   589824 | Active:     1502 (  0.3%) | Frozen:   588322\n",
            "backbone.blocks.6.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.7.norm1.weight | Total:      384 | Active:      114 ( 29.7%) | Frozen:      270\n",
            "backbone.blocks.7.norm1.bias   | Total:      384 | Active:       13 (  3.4%) | Frozen:      371\n",
            "backbone.blocks.7.attn.qkv.weight | Total:   442368 | Active:    41040 (  9.3%) | Frozen:   401328\n",
            "backbone.blocks.7.attn.qkv.bias | Total:     1152 | Active:      521 ( 45.2%) | Frozen:      631\n",
            "backbone.blocks.7.attn.proj.weight | Total:   147456 | Active:        4 (  0.0%) | Frozen:   147452\n",
            "backbone.blocks.7.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.7.norm2.weight | Total:      384 | Active:      226 ( 58.9%) | Frozen:      158\n",
            "backbone.blocks.7.norm2.bias   | Total:      384 | Active:       70 ( 18.2%) | Frozen:      314\n",
            "backbone.blocks.7.mlp.fc1.weight | Total:   589824 | Active:     7998 (  1.4%) | Frozen:   581826\n",
            "backbone.blocks.7.mlp.fc1.bias | Total:     1536 | Active:      205 ( 13.3%) | Frozen:     1331\n",
            "backbone.blocks.7.mlp.fc2.weight | Total:   589824 | Active:     1180 (  0.2%) | Frozen:   588644\n",
            "backbone.blocks.7.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.8.norm1.weight | Total:      384 | Active:      151 ( 39.3%) | Frozen:      233\n",
            "backbone.blocks.8.norm1.bias   | Total:      384 | Active:       23 (  6.0%) | Frozen:      361\n",
            "backbone.blocks.8.attn.qkv.weight | Total:   442368 | Active:    23144 (  5.2%) | Frozen:   419224\n",
            "backbone.blocks.8.attn.qkv.bias | Total:     1152 | Active:      497 ( 43.1%) | Frozen:      655\n",
            "backbone.blocks.8.attn.proj.weight | Total:   147456 | Active:        0 (  0.0%) | Frozen:   147456\n",
            "backbone.blocks.8.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.8.norm2.weight | Total:      384 | Active:      315 ( 82.0%) | Frozen:       69\n",
            "backbone.blocks.8.norm2.bias   | Total:      384 | Active:      133 ( 34.6%) | Frozen:      251\n",
            "backbone.blocks.8.mlp.fc1.weight | Total:   589824 | Active:    10261 (  1.7%) | Frozen:   579563\n",
            "backbone.blocks.8.mlp.fc1.bias | Total:     1536 | Active:      280 ( 18.2%) | Frozen:     1256\n",
            "backbone.blocks.8.mlp.fc2.weight | Total:   589824 | Active:     2751 (  0.5%) | Frozen:   587073\n",
            "backbone.blocks.8.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.9.norm1.weight | Total:      384 | Active:      163 ( 42.4%) | Frozen:      221\n",
            "backbone.blocks.9.norm1.bias   | Total:      384 | Active:       22 (  5.7%) | Frozen:      362\n",
            "backbone.blocks.9.attn.qkv.weight | Total:   442368 | Active:    13536 (  3.1%) | Frozen:   428832\n",
            "backbone.blocks.9.attn.qkv.bias | Total:     1152 | Active:      556 ( 48.3%) | Frozen:      596\n",
            "backbone.blocks.9.attn.proj.weight | Total:   147456 | Active:        0 (  0.0%) | Frozen:   147456\n",
            "backbone.blocks.9.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.9.norm2.weight | Total:      384 | Active:      381 ( 99.2%) | Frozen:        3\n",
            "backbone.blocks.9.norm2.bias   | Total:      384 | Active:      384 (100.0%) | Frozen:        0\n",
            "backbone.blocks.9.mlp.fc1.weight | Total:   589824 | Active:    81918 ( 13.9%) | Frozen:   507906\n",
            "backbone.blocks.9.mlp.fc1.bias | Total:     1536 | Active:     1327 ( 86.4%) | Frozen:      209\n",
            "backbone.blocks.9.mlp.fc2.weight | Total:   589824 | Active:   198911 ( 33.7%) | Frozen:   390913\n",
            "backbone.blocks.9.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.10.norm1.weight | Total:      384 | Active:      138 ( 35.9%) | Frozen:      246\n",
            "backbone.blocks.10.norm1.bias  | Total:      384 | Active:        2 (  0.5%) | Frozen:      382\n",
            "backbone.blocks.10.attn.qkv.weight | Total:   442368 | Active:    11979 (  2.7%) | Frozen:   430389\n",
            "backbone.blocks.10.attn.qkv.bias | Total:     1152 | Active:      559 ( 48.5%) | Frozen:      593\n",
            "backbone.blocks.10.attn.proj.weight | Total:   147456 | Active:        3 (  0.0%) | Frozen:   147453\n",
            "backbone.blocks.10.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.10.norm2.weight | Total:      384 | Active:      315 ( 82.0%) | Frozen:       69\n",
            "backbone.blocks.10.norm2.bias  | Total:      384 | Active:      223 ( 58.1%) | Frozen:      161\n",
            "backbone.blocks.10.mlp.fc1.weight | Total:   589824 | Active:    31819 (  5.4%) | Frozen:   558005\n",
            "backbone.blocks.10.mlp.fc1.bias | Total:     1536 | Active:      539 ( 35.1%) | Frozen:      997\n",
            "backbone.blocks.10.mlp.fc2.weight | Total:   589824 | Active:    27230 (  4.6%) | Frozen:   562594\n",
            "backbone.blocks.10.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.11.norm1.weight | Total:      384 | Active:      109 ( 28.4%) | Frozen:      275\n",
            "backbone.blocks.11.norm1.bias  | Total:      384 | Active:        4 (  1.0%) | Frozen:      380\n",
            "backbone.blocks.11.attn.qkv.weight | Total:   442368 | Active:    13995 (  3.2%) | Frozen:   428373\n",
            "backbone.blocks.11.attn.qkv.bias | Total:     1152 | Active:      424 ( 36.8%) | Frozen:      728\n",
            "backbone.blocks.11.attn.proj.weight | Total:   147456 | Active:        4 (  0.0%) | Frozen:   147452\n",
            "backbone.blocks.11.attn.proj.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.blocks.11.norm2.weight | Total:      384 | Active:      335 ( 87.2%) | Frozen:       49\n",
            "backbone.blocks.11.norm2.bias  | Total:      384 | Active:      307 ( 79.9%) | Frozen:       77\n",
            "backbone.blocks.11.mlp.fc1.weight | Total:   589824 | Active:    99344 ( 16.8%) | Frozen:   490480\n",
            "backbone.blocks.11.mlp.fc1.bias | Total:     1536 | Active:      797 ( 51.9%) | Frozen:      739\n",
            "backbone.blocks.11.mlp.fc2.weight | Total:   589824 | Active:   109163 ( 18.5%) | Frozen:   480661\n",
            "backbone.blocks.11.mlp.fc2.bias | Total:      384 | Active:        0 (  0.0%) | Frozen:      384\n",
            "backbone.norm.weight           | Total:      384 | Active:        6 (  1.6%) | Frozen:      378\n",
            "backbone.norm.bias             | Total:      384 | Active:        2 (  0.5%) | Frozen:      382\n",
            "head.weight                    | Total:    38400 | Active:     3582 (  9.3%) | Frozen:    34818\n",
            "head.bias                      | Total:      100 | Active:       13 ( 13.0%) | Frozen:       87\n",
            "----------------------------------------------------------------------\n",
            "总计                             | Total: 21704164 | Active:  2170416 ( 10.0%) | Frozen: 19533748\n",
            "\n",
            "✓ 掩码生成正常\n",
            "\n",
            "======================================================================\n",
            "诊断 2: 检查训练步骤\n",
            "======================================================================\n",
            "Creating IID partition for 10 clients...\n",
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "检查backbone冻结状态:\n",
            "Backbone可训练参数数: 150\n",
            "❌ 错误: Backbone应该被完全冻结!\n",
            "\n",
            "======================================================================\n",
            "诊断 3: 检查FedAvg聚合\n",
            "======================================================================\n",
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "执行聚合...\n",
            "全局模型权重最大变化: 0.325009\n",
            "✓ 聚合正常\n",
            "\n",
            "======================================================================\n",
            "诊断 4: 测试标准FedAvg (无Task Arithmetic)\n",
            "======================================================================\n",
            "Creating IID partition for 10 clients...\n",
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "运行3轮标准FedAvg...\n",
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading/Loading DINO ViT-S/16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 1: Test Acc = 6.55%\n",
            "  ✓ 精度开始提升,基础流程正常\n",
            "\n",
            "======================================================================\n",
            "诊断总结\n",
            "======================================================================\n",
            "1. 掩码生成: ✓ 正常\n",
            "2. 训练步骤: ❌ 异常\n",
            "3. FedAvg聚合: ✓ 正常\n",
            "4. 标准FedAvg: ✓ 正常\n",
            "\n",
            "======================================================================\n",
            "建议:\n",
            "======================================================================\n",
            "1. 检查模型初始化\n",
            "2. 确认backbone正确冻结\n",
            "3. 调整学习率\n",
            "\n",
            "💡 快速修复建议:\n",
            "   - 先确保标准FedAvg能work (精度>5%)\n",
            "   - 再加入Task Arithmetic\n",
            "   - 使用较大的sparsity_ratio开始测试\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}